{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the idea of a Double-Wilson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure factors of related structures are likely to be strongly correlated. We should be able to exploit this correlation during scaling and merging by constraining the optimization more strongly than by the requirement that structure factors following a Wilson distribution. \n",
    "\n",
    "The Wilson distribution derives from a model where each structure factor derives from a 1D (centric) or 2D (acentric) random walk on the real axis or the complex plane, as appropriate. The amplitudes thereof follow the Wilson distributions. A natural extension would be for the corresponding structure factors of two data sets to perform correlated random walks, resulting in a joint multivariate normal distribution of the real and complex components of the (complex) structure factors of the two data sets.\n",
    "\n",
    "We will use pairs of similar datasets to explore this idea in steps. The section numbers apply if you have a TOC enabled (they're static, so they may be outdated).\n",
    "* (1.2)   Loading data and combining them into a single frame.\n",
    "* (1.3)   Scaling to each other and normalizing data sets using an anisotropic scale model.\n",
    "* (1.4)   Normalizing structure factors using \"Kernel ridge regression\" or \"k-nearest neighbor regression\" to estimate Sigma, the mean reflection intensity within different parts of the reciprocal lattice.\n",
    "* (1.5.1) Exploring the properties of the Double-Wilson distribution for fake data drawn from the distribution.\n",
    "* (1.5.2) Calibrating the model to observed correlations for the real data\n",
    "* (1.5.3) Analysis of the real data\n",
    "\n",
    "A key point revealed by the analysis is that the parameter $r_{DW}$ used in the Double-Wilson model is a function of resolution and seems to be well described by the Luzzati model on p. 903 in Read (1990), \"Structure-Factor Probabilities for Related Structures\". \n",
    "\n",
    "**PS**: Read's paper is focused on correlations between Fc's rather than Fobs's, its entire analytical apparatus carries over to our case with \n",
    "\n",
    "- $r_{DW} = \\textbf{D}$ (or $\\sigma_E$ in eqs. 28-30).\n",
    "- his $\\textbf{s}$ equal to the reciprocal lattice point vector which I calculate below as (rs_a, rs_b, rs_c).\n",
    "- his $\\sigma^2_{\\Delta}$ related to our conditional variance $\\frac{1}{2}(1-r_{DW}^2)$.\n",
    "\n",
    "**Notation**\n",
    "- we will use $hkl$ and ```HKL```\n",
    "- we will denote reciprocal lattice point coordinates as ```(rs_a,rs_b,rs_c)``` and $r*$, with magnitude $1/d_{hkl}$ or ```dHKL```. The scattering vector $s=S_1-S_0$ equals $r*$ in our case (elastic scattering).\n",
    "\n",
    "**Possible improvements**\n",
    "* Kevin suggests using the Wilson distributions themselves as the loss functions in kernel ridge regression. This can't be implemented in Sci-kit learn, but could be in keras/Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(300000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "# !cat /proc/meminfo | grep Mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "from mpl_toolkits import mplot3d\n",
    "from tqdm         import tqdm\n",
    "from os           import path as os_path\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reciprocalspaceship as rs\n",
    "import gemmi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors       import KNeighborsRegressor\n",
    "from scipy.optimize          import least_squares, minimize\n",
    "from scipy.stats             import rice, foldnorm, vonmises, linregress, pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I will use ```ds``` as a generic dummy for a dataset used locally when that simplifies notation, e.g. in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"double_wilson_data\\\\\" # Windows\n",
    "path=\"double_wilson_data/\"    # Unix\n",
    "data_set_pairs = [(\"5kvx_phases\", \"5kw3_phases\"),\\\n",
    "                  (\"5e1y_phases\", \"2vwr_phases\"),\\\n",
    "                  (\"e35cdef_OFF200ns_HD_2sig_varEll\",\"e35cdef_200ns_HD_2sig_varEll\")]\n",
    "# example 1 fails on kNN regression\n",
    "example=2\n",
    "mtz1 = data_set_pairs[example][0] # omit .mtz\n",
    "mtz2 = data_set_pairs[example][1]\n",
    "ds1 = rs.read_mtz(path + mtz1 + \".mtz\")\n",
    "ds2 = rs.read_mtz(path + mtz2 + \".mtz\")\n",
    "print(ds1.cell)\n",
    "print(ds2.cell)\n",
    "\n",
    "ds1.info()\n",
    "ds2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'reciprocalspaceship.dataset.DataSet'>\n",
      "MultiIndex: 33298 entries, (0, 0, 4) to (63, 2, 1)\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype           \n",
      "---  ------      --------------  -----           \n",
      " 0   FreeR_flag  33298 non-null  MTZInt          \n",
      " 1   IMEAN       33281 non-null  Intensity       \n",
      " 2   SIGIMEAN    33281 non-null  Stddev          \n",
      " 3   I(+)        33281 non-null  FriedelIntensity\n",
      " 4   SIGI(+)     33281 non-null  StddevFriedelI  \n",
      " 5   I(-)        33281 non-null  FriedelIntensity\n",
      " 6   SIGI(-)     33281 non-null  StddevFriedelI  \n",
      " 7   N(+)        33281 non-null  MTZInt          \n",
      " 8   N(-)        33281 non-null  MTZInt          \n",
      "dtypes: FriedelIntensity(2), Intensity(1), MTZInt(3), Stddev(1), StddevFriedelI(2)\n",
      "memory usage: 2.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/dhekstra/miniconda3/envs/crystallography/lib/python3.8/site-packages/pandas/core/arrays/base.py:1285: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  res = [op(a, b) for (a, b) in zip(lvalues, rvalues)]\n",
      "/n/home12/dhekstra/miniconda3/envs/crystallography/lib/python3.8/site-packages/pandas/core/arrays/base.py:1285: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  res = [op(a, b) for (a, b) in zip(lvalues, rvalues)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'reciprocalspaceship.dataset.DataSet'>\n",
      "MultiIndex: 33298 entries, (0, 0, 4) to (63, 2, 1)\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   FreeR_flag  33298 non-null  MTZInt        \n",
      " 1   IMEAN       33281 non-null  Intensity     \n",
      " 2   SIGIMEAN    33281 non-null  Stddev        \n",
      " 3   I(+)        33281 non-null  float32       \n",
      " 4   SIGI(+)     33281 non-null  StddevFriedelI\n",
      " 5   I(-)        33281 non-null  float32       \n",
      " 6   SIGI(-)     33281 non-null  StddevFriedelI\n",
      " 7   N(+)        33281 non-null  MTZInt        \n",
      " 8   N(-)        33281 non-null  MTZInt        \n",
      " 9   FP_1        33281 non-null  float32       \n",
      " 10  FP_2        33281 non-null  float32       \n",
      " 11  SIGFP_1     33272 non-null  StddevFriedelI\n",
      " 12  SIGFP_2     33275 non-null  StddevFriedelI\n",
      "dtypes: Intensity(1), MTZInt(3), Stddev(1), StddevFriedelI(4), float32(4)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "path=\"double_wilson_data/\"    # Unix\n",
    "ds1=rs.read_mtz(path + \"NECAT_HEWL_RT_NaI_82_XDS_scaled1.mtz\")\n",
    "ds1.info()\n",
    "\n",
    "ds1[\"I(-)\"]=np.maximum(0,ds1[\"I(-)\"].to_numpy())\n",
    "ds1[\"I(+)\"]=np.maximum(0,ds1[\"I(+)\"].to_numpy())\n",
    "ds1[\"FP_1\"]=np.sqrt( ds1[\"I(-)\"])\n",
    "ds1[\"FP_2\"]=np.sqrt( ds1[\"I(+)\"])\n",
    "ds1[\"SIGFP_1\"] = 0.5*ds1[\"SIGI(-)\"]/ds1[\"FP_1\"]\n",
    "ds1[\"SIGFP_2\"] = 0.5*ds1[\"SIGI(+)\"]/ds1[\"FP_2\"]\n",
    "ds1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding reciprocal lattice point coordinates\n",
    "Let's calculate the reciprocal lattice point coordinates for each reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_add_rs(ds,force_rs=False,inplace=True):\n",
    "    \"\"\"\n",
    "    Adds three columns to an rs dataframe with the reciprocal space coordinates (in A^-1) for each Miller index.\n",
    "    Inplace by default!\n",
    "    \"\"\"\n",
    "    if force_rs or (not \"rs_a\" in ds.keys()):\n",
    "        orthomat_list  = ds.cell.orthogonalization_matrix.tolist()\n",
    "        orthomat = np.asarray(orthomat_list)\n",
    "    \n",
    "        hkl_array = np.asarray(list(ds.index.to_numpy()))\n",
    "    \n",
    "        orthomat_inv_t = np.linalg.inv(orthomat).transpose()\n",
    "        S = np.matmul(orthomat_inv_t, hkl_array.transpose())\n",
    "        \n",
    "        if inplace==True:\n",
    "            ds[\"rs_a\"]=S.transpose()[:,0]\n",
    "            ds[\"rs_b\"]=S.transpose()[:,1]\n",
    "            ds[\"rs_c\"]=S.transpose()[:,2]\n",
    "        else:\n",
    "            ds_out=ds.copy()\n",
    "            ds_out[\"rs_a\"]=S.transpose()[:,0]\n",
    "            ds_out[\"rs_b\"]=S.transpose()[:,1]\n",
    "            ds_out[\"rs_c\"]=S.transpose()[:,2]\n",
    "    else:\n",
    "        if inplace==True:\n",
    "            pass\n",
    "        else:\n",
    "            ds_out=ds.copy()\n",
    "            ds_out[\"rs_a\"] = ds[\"rs_a\"]\n",
    "            ds_out[\"rs_b\"] = ds[\"rs_b\"]\n",
    "            ds_out[\"rs_c\"] = ds[\"rs_c\"]\n",
    "    if inplace==True:\n",
    "        return # already done\n",
    "    else:\n",
    "        return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_add_rs(ds1)\n",
    "ds_add_rs(ds2)\n",
    "# ds_test = ds_add_rs(ds2,inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining data sets in a joint data frame\n",
    "Let's combine (\"merge\" in pandas jargon) the data frames of the two datasets. We'll add the ratios of the observed structure factor amplitudes as columns and a few other columns that will come in handy. \n",
    "Towards the end of the notebook, we clean up all datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_2= ds1.merge(ds2, left_index=True, right_index=True, suffixes=(\"_1\", \"_2\"))\n",
    "\n",
    "# Let's add some extra columns with useful info\n",
    "ds1_2[\"gamma_pp\"] = ds1_2[\"FP_2\"]/ds1_2[\"FP_1\"]\n",
    "ds1_2.compute_dHKL(        inplace=True)\n",
    "ds1_2.label_centrics(      inplace=True)\n",
    "ds1_2.compute_multiplicity(inplace=True)\n",
    "ds1_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check our reciprocal lattice coordinates by checking their norm matches $1/d_{HKL}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.sqrt(1/(ds1_2[\"rs_a_2\"]**2 +ds1_2[\"rs_b_2\"]**2 +ds1_2[\"rs_c_2\"]**2))\n",
    "print((tmp==0).any())\n",
    "\n",
    "plt.plot(np.sqrt(1/(ds1_2[\"rs_a_2\"]**2 +ds1_2[\"rs_b_2\"]**2 +ds1_2[\"rs_c_2\"]**2)), ds1_2[\"dHKL\"],'b.',alpha=0.1)\n",
    "plt.xlabel('d calculated from rlp coordinates'), plt.ylabel('dHKL from rs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking reciprocal space (ASU) coverage\n",
    "Let's see what chunk of reciprocal space these reflections are located in. I can't recall how I got this figure to be interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses \"from mpl_toolkits import mplot3d\"\n",
    "plt.ion\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.scatter3D(ds1[\"rs_a\"], ds1[\"rs_b\"], ds1[\"rs_c\"],alpha=0.01)\n",
    "ax.view_init(elev=60., azim=1200.)\n",
    "ax.set_xlabel(\"rs_a\")\n",
    "ax.set_ylabel(\"rs_b\")\n",
    "ax.set_zlabel(\"rs_c\")\n",
    "plt.draw()\n",
    "\n",
    "#for angle in range(0, 360):\n",
    "#    ax.view_init(30, angle)\n",
    "#    plt.draw()\n",
    "#    plt.pause(.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by setting a training set and test set. Because of the way the loss function below is constructed, we can just pass on entire data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses \"from sklearn.model_selection import train_test_split\"\n",
    "# the split may not be currently relevant.\n",
    "X_train, X_test, y_train, y_test = train_test_split(ds1_2, ds1_2, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "We'll define two \"loss functions\" or \"objective functions\" which we'll minimize in order to \n",
    "1. anisotropically scale one data set to another, or \n",
    "2. optimally predict average structure factor amplitude from (hkl) in the form of the reciprocal lattice point coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilson_dist_normalized(E, centric=False, nargout=1):\n",
    "    if centric:\n",
    "        P_E = np.sqrt(2/np.pi)*np.exp(-0.5*E**2)  # Rupp eq. 7-111\n",
    "        P_E2 = foldnorm.pdf(E,0,0,1)\n",
    "    else:\n",
    "        P_E = 2*E*np.exp(-E**2)                   # Rupp eq. 7-112\n",
    "        P_E2 = rice.pdf(E,0,0,np.sqrt(0.5))\n",
    "    if nargout==1:\n",
    "        return P_E\n",
    "    else:\n",
    "        return P_E, P_E2\n",
    "\n",
    "x=np.linspace(0,5,1000)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "pe1,pe2=wilson_dist_normalized(x,centric=True,nargout=2)\n",
    "plt.plot(x,pe1,'k-')\n",
    "plt.plot(x,pe2,'r--')\n",
    "plt.grid()\n",
    "plt.title(\"Centric Wilson v Folded normal\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "pe1,pe2=wilson_dist_normalized(x,centric=False,nargout=2)\n",
    "plt.plot(x,pe1,'k-')\n",
    "plt.plot(x,pe2,'r--')\n",
    "plt.grid()\n",
    "plt.title(\"Acentric Wilson v Rice\")\n",
    "plt.show()\n",
    "\n",
    "x=np.linspace(0,5,1000)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "pe1,pe2=wilson_dist_normalized(x,centric=True,nargout=2)\n",
    "plt.plot(x,np.log(pe1),'k-')\n",
    "plt.plot(x,foldnorm.logpdf(x,0,0,1),'r--')\n",
    "plt.grid()\n",
    "plt.title(\"Centric Wilson v Folded normal\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "pe1,pe2=wilson_dist_normalized(x,centric=False,nargout=2)\n",
    "plt.plot(x,np.log(pe1),'k-')\n",
    "plt.plot(x,rice.logpdf(x,0,0,np.sqrt(0.5)),'r--')\n",
    "plt.grid()\n",
    "plt.title(\"Acentric Wilson v Rice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anisotropic_scaling_train_model(p, ds_a, ds_b,nargout=1):\n",
    "    \"\"\"\n",
    "    Anisotropically scales one set of structure factor amplitudes to another (presumably one \n",
    "    can do this for intensities too). \n",
    "    \n",
    "    Assumes rs dataframes obtained from merging two data sets (in the pandas sense),\n",
    "    the first one with keys FP_1, SIGFP_1, rs_a_1, rs_b_1, rs_c_1; \n",
    "    the second with keys FP_2, SIGFP_2.\n",
    "    The first one is scaled to the second one with input parameters p such that \n",
    "    B = [[p0, p1, p2], [p1, p3, p4], [p2, p4, p5]]. The two dataframes can be the same.\n",
    "    \n",
    "    Returns an error-weighted residual.\n",
    "    \"\"\"\n",
    "    \n",
    "    rlp      = ds_a[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]].to_numpy()\n",
    "    \n",
    "    B        = np.asarray([[p[0],p[1],p[2]],[p[1],p[3],p[4]],[p[2],p[4],p[5]]])\n",
    "    B_corr   = np.exp(-np.sum(rlp * (B @ rlp.transpose()).transpose(),axis=1)).reshape(-1,1)\n",
    "    \n",
    "    FP_corr  = np.abs(p[6])*B_corr * ds_a[\"FP_1\"].to_numpy().reshape(-1,1)\n",
    "    residual = ds_b[\"FP_2\"].to_numpy().reshape(-1,1) - FP_corr\n",
    "    #print(FP_corr)\n",
    "    \n",
    "    err_a = ds_a[\"SIGFP_1\"].to_numpy()\n",
    "    err_b = ds_b[\"SIGFP_2\"].to_numpy()\n",
    "    quad_error = np.sqrt(err_a**2 + err_b**2)\n",
    "    residual = residual/quad_error.reshape(-1,1)\n",
    "    \n",
    "    if nargout == 1:\n",
    "        return residual.flatten()\n",
    "    else:\n",
    "        return residual.flatten(), FP_corr, B_corr\n",
    "\n",
    "def anisotropic_scaling_to_1(p, ds, label=\"FP_1\", mode=\"F\", nargout=1):\n",
    "    \"\"\"\n",
    "    Scales a dataset with keys FP_1, SIGFP_1, to optimally fit F = a * exp(-r*T B r*))*sqrt(eps), \n",
    "    with r* the reciprocal lattice vectors, encoded as rs_a_1, rs_b_1, rs_c_1, \n",
    "    epsilon the multiplicity, and {a * exp(-r*T B r*))} playing the role of 1/sqrt(Sigma).\n",
    "    Residuals are weighted by SIGFP, which may not be so appropriate...    \n",
    "    \n",
    "    Input arguments:\n",
    "        p :       list or vector with parameters such that B = [[p0, p1, p2], [p1, p3, p4], [p2, p4, p5]] \n",
    "                   and p[6] is a scalar prefactor for structure factor amplitudes\n",
    "        ds:       data frame with, at least, columns \"rs_a_1\", \"rs_b_1\", \"rs_c_1\", \"EPSILON\"\n",
    "        label:    column label for structure factor amplitdes to be scaled. default: \"FP_1\".\n",
    "                   a column with label {\"SIG\" + label} should also exist.\n",
    "        mode :    mode=\"F\" (default) will scale structure factor amplitudes to have amean of 1; \"I\" will do so for \n",
    "                  intensities, accounting for multiplicity.\n",
    "        nargouts: determines whether to only output residuals (1) or all return values (>1)\n",
    "        \n",
    "    Returns:\n",
    "        residual: an error-weighted residual\n",
    "        EP_corr:  scaled structure factor amplitudes\n",
    "        err_EP:   error of EP_corr\n",
    "    \"\"\"\n",
    "    \n",
    "    rlp      = ds[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]].to_numpy()\n",
    "    \n",
    "    B        = np.asarray([[p[0],p[1],p[2]],[p[1],p[3],p[4]],[p[2],p[4],p[5]]])\n",
    "    B_corr   = np.exp(-np.sum(rlp * (B @ rlp.transpose()).transpose(),axis=1))\n",
    "    F        = ds[label].to_numpy()\n",
    "    total_scale = np.abs(p[6]) * B_corr / np.sqrt(ds[\"EPSILON\"].to_numpy())\n",
    "    EP_corr  = total_scale * F\n",
    "    \n",
    "    err_F = ds[\"SIG\" + label].to_numpy()\n",
    "    err_EP  = err_F * total_scale\n",
    "    if mode==\"F\":\n",
    "        if nargout > 1:\n",
    "            print(\"mode: F\")\n",
    "        residual = (1.0 - EP_corr) # amplitudes scaled to 1 (not intensities, which would be more appropriate)\n",
    "        residual = residual/err_EP\n",
    "    else: #assume normalize as intensities\n",
    "        if nargout > 1:\n",
    "            print(\"mode: I\")\n",
    "        residual = (1.0 - EP_corr**2) # amplitudes scaled to 1 (not intensities, which would be more appropriate)\n",
    "        err_I    = 2 * F * err_F      # error propagation\n",
    "        err_EP2  = err_I * total_scale**2\n",
    "        residual = residual/err_EP2\n",
    "        \n",
    "    if nargout == 1:\n",
    "        return residual\n",
    "    else:\n",
    "        return residual, EP_corr, err_EP\n",
    "    \n",
    "def anisotropic_scaling_to_1_wilson_loss(p, ds, label=\"FP_1\", nargout=1):\n",
    "    \"\"\"\n",
    "    Scales a dataset with keys FP_1, SIGFP_1, to optimally fit F = a * exp(-r*T B r*))*sqrt(eps), \n",
    "    with r* the reciprocal lattice vectors, encoded as rs_a_1, rs_b_1, rs_c_1, \n",
    "    epsilon the multiplicity, and {a * exp(-r*T B r*))} playing the role of 1/sqrt(Sigma)\n",
    "    We'll assume that measurement error is irrelevant to observed spread.     \n",
    "    \n",
    "    Input arguments:\n",
    "        p :       list or vector with parameters such that B = [[p0, p1, p2], [p1, p3, p4], [p2, p4, p5]] \n",
    "                   and p[6] is a scalar prefactor for structure factor amplitudes\n",
    "        ds:       data frame with, at least, columns \"rs_a_1\", \"rs_b_1\", \"rs_c_1\", \"EPSILON\"\n",
    "        label:    column label for structure factor amplitdes to be scaled. default: \"FP_1\".\n",
    "                   a column with label {\"SIG\" + label} should also exist.\n",
    "        nargouts: determines whether to only output residuals (1) or all return values (>1)\n",
    "        \n",
    "    Returns:\n",
    "        loss:     loss function value\n",
    "        EP_corr:  scaled structure factor amplitudes\n",
    "        err_EP:   error of EP_corr\n",
    "    \"\"\"\n",
    "    \n",
    "    rlp      = ds[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]].to_numpy()\n",
    "    \n",
    "    B        = np.asarray([[p[0],p[1],p[2]],[p[1],p[3],p[4]],[p[2],p[4],p[5]]])\n",
    "    B_corr   = np.exp(-np.sum(rlp * (B @ rlp.transpose()).transpose(),axis=1))\n",
    "    F        = ds[label].to_numpy()\n",
    "    total_scale = np.exp(p[6]) * B_corr / np.sqrt(ds[\"EPSILON\"].to_numpy())\n",
    "    EP_corr  = total_scale * F\n",
    "    err_F = ds[\"SIG\" + label].to_numpy()\n",
    "    err_EP  = err_F * total_scale\n",
    "\n",
    "    EP_corr  = (EP_corr**4/(1+(EP_corr/20)**4))**0.25 # we need this nonlinearity to stop running out of the range \n",
    "                                            # where the loss functions have meaningfull probability\n",
    "    loss_E_ac = -2*rice.logpdf(EP_corr, 0,0,np.sqrt(0.5))\n",
    "    loss_E_c  = -2*foldnorm.logpdf(EP_corr, 0, 0, 1)\n",
    "\n",
    "    loss = np.sum(loss_E_ac[ds[\"CENTRIC\"]==False]) + np.sum(loss_E_c[ds[\"CENTRIC\"]==True])\n",
    "    \n",
    "    if nargout == 1:\n",
    "        return loss\n",
    "    else:\n",
    "        return loss, EP_corr, err_EP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we use the following soft upper limit above\n",
    "x=np.linspace(0,40,1000)\n",
    "plt.plot(x,x,'k-')\n",
    "plt.plot(x,np.sqrt(x**2/(1+0.0025*x**2)),'b-')\n",
    "plt.plot(x,(x**4/(1+(x/20)**4))**0.25,'g-')\n",
    "plt.plot(x,x/(1+0.05*x),'r-')\n",
    "plt.text(13,18,\"y=x\")\n",
    "plt.ylim([0,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: scaling data set 1 to data set 2\n",
    "Next, we set up a robust nonlinear least-squares fit of the anisotropic model above to scale dataset 1 to dataset 2. That is, \n",
    "$F_1^{sc}= A e^{-r*^{T} B r*} F_1$, with $r*$ the reciprocal lattice point coordinates which depend on $hkl$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import least_squares\n",
    "res_robust   = least_squares(anisotropic_scaling_train_model, [1,0,0,1,0,1,1], loss='cauchy', f_scale=1, args=(X_train, y_train))\n",
    "\n",
    "# with a round of outlier rejection:\n",
    "#res_robust_2 = least_squares(anisotropic_scaling_model, res_robust.x,  loss='soft_l1', f_scale=1, \n",
    "#                             args=(X_train.loc[np.abs(res_robust.fun)<np.percentile(np.abs(res_robust.fun),98)], \n",
    "#                                   y_train.loc[np.abs(res_robust.fun)<np.percentile(np.abs(res_robust.fun),98)]))\n",
    "\n",
    "plt.hist(res_robust.fun,100, alpha=0.3,color='b', density=True)\n",
    "plt.grid()\n",
    "plt.xlabel(\"final residual for anisotropic scaling of |F1| to |F2|\")\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well ratios of |F_2|/|F_1| are explainted by the anisotropic scaling coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "test,  ep_corr,  B_corr = anisotropic_scaling_train_model(res_robust.x, X_test, y_test, nargout=3)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(B_corr.flatten(), y_test[\"gamma_pp\"].to_numpy())\n",
    "print(r_value)\n",
    "print(\"Slope: \" + str(slope) + \", intercept: \" + str(intercept))\n",
    "\n",
    "plt.hist(B_corr,100)\n",
    "plt.xlabel(\"B-factor-based correction factor\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(B_corr, y_test[\"gamma_pp\"].to_numpy(),'b.',alpha=0.1)\n",
    "plt.xlabel('Expected structure factor ratio from aniso scaling')\n",
    "plt.ylabel('Observed structure factor ratio')\n",
    "# plt.ylim((0,plt.xlim()[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most examples, anisotropic scaling explains a significant part of the observed ratios of structure factor amplitudes. The **e35**  example is an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding scaled FP1 to the joint dataframe\n",
    "Adding the anisotropically scaled FP_1 and SIGFP_1 to the joint dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2, fp_corr2, B_corr2 = anisotropic_scaling_train_model(res_robust.x, ds1_2, ds1_2, nargout=3)\n",
    "ds1_2[\"FP_1_scaled\"] = fp_corr2.flatten()\n",
    "ds1_2[\"SIGFP_1_scaled\"] = B_corr2.flatten() * ds1_2[\"SIGFP_1\"]\n",
    "# ds1_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of anisotropic scaling is usually clear graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,4))\n",
    "plt.subplot(1,2, 1)\n",
    "plt.loglog(ds1_2[\"FP_1\"], ds1_2[\"FP_2\"],'k.',alpha=0.02)\n",
    "plt.xlabel(\"FP_1\"), plt.ylabel(\"FP_2\")\n",
    "plt.plot(plt.xlim(),plt.xlim(),'r-')\n",
    "\n",
    "plt.subplot(1,2, 2)\n",
    "plt.loglog(ds1_2[\"FP_1_scaled\"], ds1_2[\"FP_2\"],'k.',alpha=0.02)\n",
    "plt.xlabel(\"FP_1 (aniso sc)\")\n",
    "plt.plot(plt.xlim(),plt.xlim(),'r-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing both datasets by anisotropic scaling\n",
    "Let's scale both sets of structure factor amplitudes to have local mean of about 1. I initially used a least-squares regression with robust loss function for this, but have replaced this with a Wilson distribution loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_robust   = least_squares(anisotropic_scaling_to_1, [1,0,0,1,0,1,1], loss='soft_l1', f_scale=1, \\\n",
    "#                              args=(ds1_2,\"FP_1\",\"I\",1)) #somehow can't keyword this\n",
    "\n",
    "res = minimize(anisotropic_scaling_to_1_wilson_loss, [1,0,0,1,0,1,1],method='nelder-mead',args=(ds1_2,\"FP_1\"))\n",
    "\n",
    "res_test, ep1, sigep1 = anisotropic_scaling_to_1_wilson_loss(res.x, ds1_2, label=\"FP_1\", nargout=3)\n",
    "ds1_2[\"EP_1_aniso\"]   = ep1\n",
    "ds1_2[\"SIGEP_1_aniso\"]= sigep1\n",
    "\n",
    "res = minimize(anisotropic_scaling_to_1_wilson_loss, [1,0,0,1,0,1,1],method='nelder-mead',args=(ds1_2,\"FP_2\"))\n",
    "res_test, ep2, sigep2 = anisotropic_scaling_to_1_wilson_loss(res.x, ds1_2, label=\"FP_2\",nargout=3)\n",
    "ds1_2[\"EP_2_aniso\"]   = ep2\n",
    "ds1_2[\"SIGEP_2_aniso\"]= sigep2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(0,3,100)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(ep1[ds1_2[\"CENTRIC\"]==True],50,alpha=0.5,density=True)\n",
    "plt.hist(ep2[ds1_2[\"CENTRIC\"]==True],50,alpha=0.5,density=True)\n",
    "plt.plot(x,np.sqrt(2)*wilson_dist_normalized(np.sqrt(2)*x,centric=True),'r-') # I am baffled by the need for the \n",
    "                                                                              # extra sqrt(2)'s\n",
    "plt.legend([\"EP1\", \"EP2\"])\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"Normalized sf amplitude\")\n",
    "plt.title(\"Centric\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(ep1[ds1_2[\"CENTRIC\"]==False],50,alpha=0.5,density=True)\n",
    "plt.hist(ep2[ds1_2[\"CENTRIC\"]==False],50,alpha=0.5,density=True)\n",
    "plt.plot(x, np.sqrt(2)*wilson_dist_normalized(np.sqrt(2)*x,centric=False),'r-')\n",
    "plt.legend([\"EP1\", \"EP2\"])\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"Normalized sf amplitude\")\n",
    "plt.title(\"Acentric\")\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(ep1[ds1_2[\"CENTRIC\"]==True]**2))\n",
    "print(np.mean(ep2[ds1_2[\"CENTRIC\"]==True]**2))\n",
    "print(np.mean(ep1[ds1_2[\"CENTRIC\"]==False]**2))\n",
    "print(np.mean(ep2[ds1_2[\"CENTRIC\"]==False]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of a symmetric loss function obviously is not great for our purpose!  On another note, for the second example (2VWR, 5E1Y) there is a striking inconsistency between the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation coefficients with scaled and unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meanF_byres(ds, label=\"FP\", nbins=20, sigma_cut=0, median=False):\n",
    "    '''Calculate mean structure factor amplitude by res. bin.\n",
    "    Use this function only for graphical inspection, not for scaling.\n",
    "    '''\n",
    "    #print(ds.shape)\n",
    "    if sigma_cut > 0:\n",
    "        incl_criteria = ds[label].to_numpy().flatten() > sigma_cut * ds[\"SIG\" + label].to_numpy().flatten()\n",
    "        ds2 = ds[incl_criteria].copy()\n",
    "    else:\n",
    "        ds2=ds.copy()\n",
    "    \n",
    "    ds2, bin_labels = ds2.assign_resolution_bins(bins=nbins)\n",
    "    if median:\n",
    "        print(\"Average observations per bin: \" + str(ds2[\"bin\"].value_counts().median()))\n",
    "        result = ds2.groupby(\"bin\")[label].median()\n",
    "    else:\n",
    "        print(\"Average observations per bin: \" + str(ds2[\"bin\"].value_counts().mean()))\n",
    "        result = ds2.groupby(\"bin\")[label].mean()\n",
    "    return result, bin_labels\n",
    "\n",
    "\n",
    "def compute_cc(ds, labels=[\"F1\",\"F2\"], nbins=20, method=\"spearman\"):\n",
    "    ds, bin_labels = ds.assign_resolution_bins(bins=nbins) #This adds a column to the input!\n",
    "    print(\"Average observations per bin: \" + str(ds[\"bin\"].value_counts().mean()))\n",
    "    g = ds.groupby(\"bin\")[labels]\n",
    "    result = g.corr(method=method).unstack().loc[:, (labels[0],labels[1])]\n",
    "    return result, bin_labels\n",
    "\n",
    "\n",
    "def plot_by_res_bin(result, bin_labels, ylabel=r\"$CC_{1/2}$\",color='b'):\n",
    "    plt.plot(result, label=\"Data\", color=color)\n",
    "    plt.xticks(result.index, bin_labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(r\"Resolution Bin ($\\AA$)\")\n",
    "    #plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    #plt.ylim(0, )\n",
    "    plt.grid(linestyle='--')\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the average SF amplitude by resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, bin_labels = compute_meanF_byres(ds1_2, label=\"FP_1\", nbins=20,sigma_cut=0)\n",
    "plot_by_res_bin(result, bin_labels, ylabel=\"mean F\",color='b')\n",
    "\n",
    "result, bin_labels = compute_meanF_byres(ds1_2, label=\"FP_1_scaled\", nbins=20,sigma_cut=0)\n",
    "plot_by_res_bin(result, bin_labels, ylabel=\"mean F\",color='r')\n",
    "\n",
    "result, bin_labels = compute_meanF_byres(ds1_2, label=\"FP_2\", nbins=20,sigma_cut=0)\n",
    "plot_by_res_bin(result, bin_labels, ylabel=\"mean F\",color='g')\n",
    "\n",
    "plt.title(\"Mean |F|\")\n",
    "plt.legend([\"FP1\", \"FP1(aniso sc)\", \"FP2\"])\n",
    "plt.ylim(0,)\n",
    "plt.show()\n",
    "\n",
    "result, bin_labels = compute_meanF_byres(ds1_2, label=\"EP_1_aniso\", nbins=20,sigma_cut=0)\n",
    "plot_by_res_bin(result, bin_labels, ylabel=\"mean E\",color='b')\n",
    "result, bin_labels = compute_meanF_byres(ds1_2, label=\"EP_2_aniso\", nbins=20,sigma_cut=0)\n",
    "plot_by_res_bin(result, bin_labels, ylabel=\"mean E\",color='r')\n",
    "plt.title(\"Mean |F|\")\n",
    "plt.legend([\"EP_1_aniso\"])\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1, labels_1 = compute_cc(ds1_2,labels=[\"FP_1\",\"FP_2\"],nbins=20)\n",
    "result_2, labels_2 = compute_cc(ds1_2,labels=[\"FP_1_scaled\",\"FP_2\"],nbins=20)\n",
    "result_3, labels_3 = compute_cc(ds1_2,labels=[\"EP_1_aniso\",\"EP_2_aniso\"],nbins=20)\n",
    "plot_by_res_bin(result_1, labels_1,)\n",
    "plot_by_res_bin(result_2, labels_2,color='r')\n",
    "plot_by_res_bin(result_3, labels_3,color='g')\n",
    "plt.legend([\"F1,F2\",\"F1 (aniso sc), F2\",\"EP_1_aniso, EP_2_aniso\"])\n",
    "# plt.ylim(0,1)\n",
    "plt.ylabel(\"CC ()\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression estimates of Sigma\n",
    "We're estimating $<|F|^2>$ locally in reciprocal space! Because the expected intensity is the same for acentric and centric reflections, we can do this in one go for all of them.\n",
    "\n",
    "By Rupp eq. (7-104): $ \\Sigma_N = \\left<I\\right>/\\varepsilon_h $\n",
    "\n",
    "Below are two implementations of $\\Sigma$ estimation: using kernel ridge regression and k-nearest neighbors with custom weights. The former retains more of the correlation among data sets than the latter, but is **much** slower and more memory-intensive. It crashes on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ds1_2[\"EPSILON\"][ds1_2[\"CENTRIC\"].to_numpy()==True]\n",
    "print(\"Distribution of multiplicities for centric reflections:\")\n",
    "print(temp.value_counts())\n",
    "print(\"\")\n",
    "\n",
    "temp = ds1_2[\"EPSILON\"][ds1_2[\"CENTRIC\"].to_numpy()==False]\n",
    "print(\"Distribution of multiplicities for acentric reflections:\")\n",
    "print(temp.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel ridge regression: see https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html\n",
    "default loss function is the MSE. GridsearchCV performs, by default, 5-fold cross-validation for each set of parameters. Because of this, we use the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps it's worth making this a method.\n",
    "def sigma_cut_ds(ds, F_col=\"FP\", sigF_col=\"SIGFP\", sigma_cut=3,inplace=False):\n",
    "    if sigma_cut > 0:\n",
    "        incl_criteria = ds[F_col].to_numpy().flatten() > sigma_cut * ds[sigF_col].to_numpy().flatten()\n",
    "        ds2 = ds.loc[incl_criteria,:].copy()\n",
    "    else:\n",
    "        ds2=ds.copy()\n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine if we can use the results from a previous kernel ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os_path.exists(path + mtz1 + \"_krr.pkl\") & os_path.exists(path + mtz2 + \"_krr.pkl\"):\n",
    "    print(\"Strongly consider skipping the next cell! It takes 30-60 min without crossvalidation\")\n",
    "    try:\n",
    "        kr_1 = pickle.load(open(path + mtz1 + \"_krr.pkl\", 'rb'))\n",
    "        kr_2 = pickle.load(open(path + mtz2 + \"_krr.pkl\", 'rb'))\n",
    "    except:\n",
    "        print(\"That didn't work\")\n",
    "else:\n",
    "    print(\"No kernel ridge-regression has been performed yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression implementation\n",
    "The next cell takes up a ton of memory on the cluster. It runs slowly-but-fine on my laptop.\n",
    "It's better to use the kNN version below instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# #from sklearn.model_selection import learning_curve\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# # consider adding pre-processing\n",
    "# gridsearch = False # it takes up to an hour\n",
    "# # this will take a few minutes:\n",
    "# # GridsearchCV \n",
    "# #   1st try   (alpha=1,  gamma=100, kernel='rbf'), with alpha and gamma at the top of the initial ranges.\n",
    "# #   2nd try   (alpha=10, gamma=100, kernel='rbf'), with alpha, but not gamma, at the top of the range\n",
    "# #   3rd try   (alpha=30, gamma=100, kernel='rbf'), with alpha in [10,30,100], gamma fixed.\n",
    "# # on 3rd try switched to neg_mean_absolute_error (from MSE)\n",
    "\n",
    "# if gridsearch: # for regression hyperparameter optimization -- needs some tweaking\n",
    "#     kr_1 = GridSearchCV(KernelRidge(kernel='rbf'), scoring='neg_mean_absolute_error',\n",
    "#                       param_grid={\"alpha\": [300, 100, 30, 10],\n",
    "#                                   \"gamma\": [300, 100, 30, 10]})\n",
    "#     kr_1.fit(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]], (ds1_2[[\"FP_1\"]].to_numpy()**2)/ds1_2[[\"EPSILON\"]].to_numpy())\n",
    "#     kr_2 = GridSearchCV(KernelRidge(kernel='rbf'), scoring='neg_mean_absolute_error',\n",
    "#                       param_grid={\"alpha\": [300, 100, 30, 10],\n",
    "#                                   \"gamma\": [300, 100, 30, 10]})\n",
    "#     kr_2.fit(ds1_2[[\"rs_a_2\", \"rs_b_2\", \"rs_c_2\"]], (ds1_2[[\"FP_2\"]].to_numpy()**2)/ds1_2[[\"EPSILON\"]].to_numpy())\n",
    "# else:\n",
    "#     kr_1 = KernelRidge(kernel='rbf', alpha=30, gamma=100)\n",
    "#     kr_2 = KernelRidge(kernel='rbf', alpha=30, gamma=100)\n",
    "#     kr_1.fit(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]], (ds1_2[[\"FP_1\"]].to_numpy()**2)/ds1_2[[\"EPSILON\"]].to_numpy())\n",
    "#     kr_2.fit(ds1_2[[\"rs_a_2\", \"rs_b_2\", \"rs_c_2\"]], (ds1_2[[\"FP_2\"]].to_numpy()**2)/ds1_2[[\"EPSILON\"]].to_numpy())\n",
    "\n",
    "# pickle.dump(kr_1, open(path + mtz1 + \"_krr.pkl\",'wb'))\n",
    "# pickle.dump(kr_2, open(path + mtz2 + \"_krr.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbor implementation to estimate Sigma\n",
    "Here is a k-nearest-neighbor variant. We'll explore a few different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-weights set to 0\n",
    "def knn_weight_norm_p01(delta_r):\n",
    "    r0 = 0.01 # inverse Angstrom\n",
    "    return (delta_r>1e-8)*np.exp(-0.5*(delta_r/r0)**2)\n",
    "\n",
    "def knn_weight_exp_p01(delta_r):\n",
    "    r0 = 0.01 # inverse Angstrom\n",
    "    return (delta_r>1e-8)*np.exp(-delta_r/r0)\n",
    "\n",
    "def knn_weight_norm_p02(delta_r):\n",
    "    r0 = 0.02 # inverse Angstrom\n",
    "    return (delta_r>1e-8)*np.exp(-0.5*(delta_r/r0)**2)\n",
    "\n",
    "def knn_weight_exp_p02(delta_r):\n",
    "    r0 = 0.02 # inverse Angstrom\n",
    "    return (delta_r>1e-8)*np.exp(-delta_r/r0)\n",
    "\n",
    "def knn_weight_norm_p03(delta_r):\n",
    "    r0 = 0.03 # inverse Angstrom\n",
    "    return (delta_r>1e-8)*np.exp(-0.5*(delta_r/r0)**2)\n",
    "\n",
    "def knn_weight_exp_p03(delta_r):\n",
    "    r0 = 0.03 # inverse Angstrom\n",
    "    return (delta_r>1e-8)*np.exp(-delta_r/r0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is meant to help debug k-nearest neighbors. Oddly the code worked for the first example and fails on the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"total NaN's or nulls in dataframe: \" + str(ds1_2.isnull().sum().sum()))\n",
    "# print(\"find rows with Infs:\")\n",
    "# print(ds1_2[(ds1_2==np.inf).any(axis=1)])\n",
    "# print(\"find rows with large FP_1:\")\n",
    "# print(ds1_2.loc[ds1_2[\"FP_1\"]>1e4,\"FP_1\"])\n",
    "# print(ds1_2.loc[ds1_2[\"FP_2\"]>1e4,\"FP_2\"])\n",
    "# print(np.isfinite(ds1_2[\"FP_1\"].to_numpy()).all())\n",
    "# print(np.isfinite(ds1_2[\"FP_2\"].to_numpy()).all())\n",
    "# print((ds1_2[\"EPSILON\"].to_numpy()>0).all())\n",
    "# tmp=(ds1_2[[\"FP_2\"]].to_numpy()**2)/(ds1_2[[\"EPSILON\"]].to_numpy())\n",
    "# np.sum(np.isnan(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know why the following cell sometimes fails. A weird divide by zero error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# consider adding pre-processing\n",
    "# grid search takes a few minutes\n",
    "gridsearch = True \n",
    "param_grid={\"n_neighbors\":[20,40,60,80,100],'weights':['uniform',knn_weight_exp_p01, knn_weight_norm_p01, \\\n",
    "                                                                 knn_weight_exp_p02, knn_weight_norm_p02, \\\n",
    "                                                                 knn_weight_exp_p03, knn_weight_norm_p03]}\n",
    "if gridsearch: # for regression hyperparameter optimization -- needs some tweaking\n",
    "    knn_1 = GridSearchCV(KNeighborsRegressor(n_jobs=8),param_grid=param_grid)\n",
    "    knn_1.fit(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]], (ds1_2[\"FP_1\"]**2/ds1_2[\"EPSILON\"]).to_numpy())\n",
    "#     knn_1.fit(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]].to_numpy(), (ds1_2[\"EP_1_aniso\"]**2).to_numpy()) # these should be corrected for eps already\n",
    "    knn_2 = GridSearchCV(KNeighborsRegressor(n_jobs=8), param_grid=param_grid)\n",
    "    knn_2.fit(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]], (ds1_2[\"FP_2\"]**2/ds1_2[\"EPSILON\"]).to_numpy())\n",
    "#     knn_2.fit(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]].to_numpy(), (ds1_2[\"EP_2_aniso\"]**2).to_numpy())\n",
    "else:\n",
    "    knn_1 = KNeighborsRegressor(40, weights=knn_weight_p02,n_jobs=8)\n",
    "    knn_1.fit(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]], (ds1_2[[\"FP_1\"]].to_numpy()**2)/ds1_2[[\"EPSILON\"]].to_numpy())\n",
    "    knn_2 = KNeighborsRegressor(40, weights=knn_weight_p02,n_jobs=8)\n",
    "    knn_2.fit(ds1_2[[\"rs_a_2\", \"rs_b_2\", \"rs_c_2\"]], (ds1_2[[\"FP_2\"]].to_numpy()**2)/ds1_2[[\"EPSILON\"]].to_numpy())\n",
    "\n",
    "pickle.dump(knn_1, open(path + mtz1 + \"_knn.pkl\",'wb'))\n",
    "pickle.dump(knn_2, open(path + mtz2 + \"_knn.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gridsearch:\n",
    "    print(knn_1.best_params_)\n",
    "    print(knn_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate normalized structure factors\n",
    "Note: we've estimated $\\Sigma$. To get normalized structure factors, $E_h=\\frac{F_h}{\\sqrt{\\varepsilon \\Sigma_h}}$, we need to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using kernel ridge regression...\n",
    "# Sigma_kr_1 = kr_1.predict(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]])\n",
    "# Sigma_kr_2 = kr_2.predict(ds1_2[[\"rs_a_2\", \"rs_b_2\", \"rs_c_2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_1 = knn_1.predict(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]])\n",
    "Sigma_2 = knn_2.predict(ds1_2[[\"rs_a_2\", \"rs_b_2\", \"rs_c_2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E1    = ds1_2[\"FP_1\"   ].to_numpy()/np.sqrt(Sigma_1 * ds1_2[\"EPSILON\"].to_numpy())\n",
    "E2    = ds1_2[\"FP_2\"   ].to_numpy()/np.sqrt(Sigma_2 * ds1_2[\"EPSILON\"].to_numpy())\n",
    "SIGE1 = ds1_2[\"SIGFP_1\"].to_numpy()/np.sqrt(Sigma_1 * ds1_2[\"EPSILON\"].to_numpy())\n",
    "SIGE2 = ds1_2[\"SIGFP_2\"].to_numpy()/np.sqrt(Sigma_2 * ds1_2[\"EPSILON\"].to_numpy())\n",
    "\n",
    "ds1_2[\"EP_1\"]    = E1\n",
    "ds1_2[\"EP_2\"]    = E2\n",
    "ds1_2[\"SIGEP_1\"] = SIGE1\n",
    "ds1_2[\"SIGEP_2\"] = SIGE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what, if anything, happened to the correlations per resolution shell (we see that kNN is a little worse than kernel ridge regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1, labels_1 = compute_cc(ds1_2,labels=[\"FP_1\",\"FP_2\"],nbins=20)\n",
    "result_2, labels_2 = compute_cc(ds1_2,labels=[\"FP_1_scaled\",\"FP_2\"],nbins=20)\n",
    "result_3, labels_3 = compute_cc(ds1_2,labels=[\"EP_1\",\"EP_2\"],nbins=20)\n",
    "plot_by_res_bin(result_1, labels_1)\n",
    "plot_by_res_bin(result_2, labels_2,color='r')\n",
    "plot_by_res_bin(result_3, labels_3,color='g')\n",
    "plt.ylabel(\"CC between datasets (Spearman)\")\n",
    "plt.legend([\"FP1-FP2\", \"FP1(sc)-FP2\", \"EP1-EP2\"])\n",
    "# plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization condenses the range of the structure factor amplitudes quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,4))\n",
    "plt.subplot(1,2, 1)\n",
    "plt.loglog(ds1_2[\"FP_1\"], ds1_2[\"FP_2\"],'k.',alpha=0.02)\n",
    "plt.xlabel(\"FP_1\"), plt.ylabel(\"FP_2\")\n",
    "xl=plt.xlim()\n",
    "plt.ylim(xl)\n",
    "plt.plot(xl,xl,'r-')\n",
    "\n",
    "plt.subplot(1,2, 2)\n",
    "plt.loglog(ds1_2[\"EP_1\"], ds1_2[\"EP_2\"],'k.',alpha=0.01)\n",
    "plt.xlabel(\"EP_1\")\n",
    "xl2=plt.xlim()\n",
    "plt.plot(xl2,xl2,'r-')\n",
    "plt.xlim((xl2[0],xl2[0]*xl[1]/xl[0])) # force to have the same range as the left panel\n",
    "plt.ylim((xl2[0],xl2[0]*xl[1]/xl[0])) # force to have the same range as the left panel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the structure factor amplitudes fit the Wilson distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins=np.linspace(0,5,50)\n",
    "\n",
    "xl=(0,5)\n",
    "plt.figure(1,figsize=(10,10))\n",
    "plt.subplot(2,2, 1)\n",
    "plt.hist(ds1_2[\"EP_1\"][ds1_2[\"CENTRIC\"].to_numpy()==False],bins=bins,density=True)\n",
    "plt.plot(bins, wilson_dist_normalized(bins, centric=False),'r-')\n",
    "plt.title(\"Distribution of acentric E\")\n",
    "plt.xlabel(\"E\")\n",
    "plt.xlim(xl)\n",
    "\n",
    "plt.subplot(2,2, 2)\n",
    "plt.hist(ds1_2[\"EP_1\"][ds1_2[\"CENTRIC\"].to_numpy()==True],bins=bins,density=True)\n",
    "plt.plot(bins, wilson_dist_normalized(bins, centric=True),'r-')\n",
    "plt.title(\"Distribution of centric E\")\n",
    "plt.xlabel(\"E\")\n",
    "plt.legend([\"Expected\",\"Observed\"])\n",
    "plt.xlim(xl)\n",
    "\n",
    "plt.subplot(2,2, 3)\n",
    "plt.hist(ds1_2[\"EP_2\"][ds1_2[\"CENTRIC\"].to_numpy()==False],bins=bins,density=True)\n",
    "plt.plot(bins, wilson_dist_normalized(bins, centric=False),'r-')\n",
    "plt.title(\"Distribution of acentric E\")\n",
    "plt.xlabel(\"E\")\n",
    "plt.xlim(xl)\n",
    "\n",
    "plt.subplot(2,2, 4)\n",
    "plt.hist(ds1_2[\"EP_2\"][ds1_2[\"CENTRIC\"].to_numpy()==True],bins=bins,density=True)\n",
    "plt.plot(bins, wilson_dist_normalized(bins, centric=True),'r-')\n",
    "plt.title(\"Distribution of centric E\")\n",
    "plt.xlabel(\"E\")\n",
    "plt.legend([\"Expected\",\"Observed\"])\n",
    "plt.xlim(xl)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: The deviations at low E seem to be attributable to weak reflections tending to be systematically excluded from the data and/or an artefact from French-Wilson scaling inflating reflections. We calculate completeness at the end of this notebook, but it does not seem to be a major explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional PDFs for normalized structure factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks using synthetic data\n",
    "We'll first examine some synthetic data generated from the Double-Wilson distribution and study their properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping from double-wilson $r$ to Pearson corr for $|E|$\n",
    "We start by making sure we understand the mapping between the parameter $r$ in the Double-Wilson distribution and the actual correlation coefficient between pairs of structure factor amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples= 500000\n",
    "nbin_r  = 20\n",
    "r       = np.linspace(0,1,nbin_r)   # correlation between random walks \n",
    "mean    = [0,0,0,0]                 # mean of random walks\n",
    "\n",
    "corr_r_ac = np.zeros([nbin_r,1])    # correlation between |F_1| and |F_2| samples from DW (acentric)\n",
    "for i in tqdm(range(nbin_r)):\n",
    "    cov     = 0.5*np.asarray([[1,0,r[i],0],[0,1,0,r[i]],[r[i],0,1,0],[0,r[i],0,1]])\n",
    "    E_1x_1y_2x_2y = np.random.multivariate_normal(mean=mean, cov=cov,size=nsamples)\n",
    "    # fake data set (acentric)\n",
    "    ep1_test = np.sqrt(E_1x_1y_2x_2y[:,0]**2 +E_1x_1y_2x_2y[:,1]**2 )\n",
    "    ep2_test = np.sqrt(E_1x_1y_2x_2y[:,2]**2 +E_1x_1y_2x_2y[:,3]**2 )\n",
    "    corr_r_ac[i] = pearsonr(ep1_test, ep2_test)[0]\n",
    "\n",
    "print(\"variance of sf acentric amplitudes from the DW distribution:\" + \\\n",
    "     str(np.var(np.concatenate((ep1_test,ep2_test)))))\n",
    "    \n",
    "corr_r_c = np.zeros([nbin_r,1])\n",
    "for i in tqdm(range(nbin_r)):\n",
    "    cov     = 0.5*np.asarray([[1,0,r[i],0],[0,1,0,r[i]],[r[i],0,1,0],[0,r[i],0,1]])\n",
    "    E_1x_1y_2x_2y = np.random.multivariate_normal(mean=mean, cov=cov,size=nsamples)\n",
    "    # fake data set (centric)\n",
    "    ep1_test = np.sqrt(E_1x_1y_2x_2y[:,0]**2 +E_1x_1y_2x_2y[:,0]**2 )\n",
    "    ep2_test = np.sqrt(E_1x_1y_2x_2y[:,2]**2 +E_1x_1y_2x_2y[:,2]**2 )\n",
    "    corr_r_c[i] = pearsonr(ep1_test, ep2_test)[0]\n",
    "\n",
    "print(\"variance of sf acentric amplitudes from the DW distribution:\" + \\\n",
    "     str(np.var(np.concatenate((ep1_test,ep2_test)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not _a priori_ obvious what the relationship between the parameter $r$ above and the correlation coefficient of the sf amplitudes should be. Below we'll see that $\\rho(|F_1|,|F_2|)\\approx r^2$. In the next two cells we look more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import lstsq\n",
    "M = r[:,np.newaxis]**[2,4,6,8]\n",
    "beta_ac, res, rnk, s = lstsq(M,corr_r_ac)\n",
    "beta_c,  res, rnk, s = lstsq(M,corr_r_c )\n",
    "\n",
    "def r_dw_to_r_E(r,beta):\n",
    "    X=np.array([r**2,r**4,r**6,r**8]).transpose()\n",
    "    return np.dot(X,beta)\n",
    "\n",
    "# despite this function, we'll use the square root as our proxy as it is trivial to invert.\n",
    "# the coefficients of the least-square fit are quite variable, even with 500,000 samples.\n",
    "\n",
    "print(beta_ac.transpose())\n",
    "print(beta_c.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)                       # Empirical and least-squares-fit mapping from r to pearson(E1,E2)\n",
    "plt.plot(r,corr_r_ac,'bo-')              # ACENTRIC\n",
    "plt.plot(r,r_dw_to_r_E(r,beta_ac),'m-')\n",
    "plt.plot(r,r**2,'r--')\n",
    "plt.legend([\"observed\",\"LSQ fit\",\"simple r^2\"])\n",
    "plt.xlabel(\"DW parameter $r$\")\n",
    "plt.ylabel(\"Pearson $r$(E1,E2; acentric)\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)                      # CENTRIC\n",
    "plt.plot(r,corr_r_c,'bo-')\n",
    "plt.plot(r,r_dw_to_r_E(r,beta_c),'m-')\n",
    "plt.plot(r,r**2,'r--')\n",
    "plt.legend([\"observed\",\"LSQ fit\",\"simple r^2\"])\n",
    "plt.xlabel(\"DW parameter $r$\")\n",
    "plt.ylabel(\"Pearson $r$(E1,E2; centric)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# now the inverse\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)                       # Empirical and least-squares-fit mapping from r to pearson(E1,E2)\n",
    "plt.plot(corr_r_ac,r,'bo-')              # ACENTRIC\n",
    "plt.plot(corr_r_ac, np.sqrt(corr_r_ac+0.15*(corr_r_ac**2-corr_r_ac**3)),'m-')\n",
    "plt.plot(corr_r_ac, np.sqrt(corr_r_ac),'r--')\n",
    "plt.legend([\"observed\",\"manual fit\",\"simple sqrt\"])\n",
    "plt.ylabel(\"DW parameter $r$\")\n",
    "plt.xlabel(\"Pearson $r$(E1,E2; acentric)\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)                       # CENTRIC\n",
    "plt.plot(corr_r_c,r,'bo-')\n",
    "plt.plot(corr_r_c, np.sqrt(corr_r_c+0.3*(corr_r_c**2-corr_r_c**3)),'m-')\n",
    "plt.plot(corr_r_c, np.sqrt(corr_r_c),'r--')\n",
    "plt.legend([\"observed\",\"manual fit\",\"simple sqrt\"])\n",
    "plt.ylabel(\"DW parameter $r$\")\n",
    "plt.xlabel(\"Pearson $r$(E1,E2; centric)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In good approximation, when a pair of normalized structure factors is drawn from an acentric double-wilson distribution, they have a correlation coefficient $r^2_{DW}$, with $r_{DW}$ parametrizing the DW distribution. This approximation is slightly worse for centric reflections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE DATA: generation & graphical inspection (acentric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples= 100000\n",
    "nbin    = 25\n",
    "x       = np.linspace(0,5,100) # x-coordinates for plots\n",
    "r_fake  = 0.9                  # correlation between random walks \n",
    "r       = r_fake\n",
    "mean    = [0,0,0,0]            # mean of random walks\n",
    "cov     = 0.5*np.asarray([[1,0,r,0],[0,1,0,r],[r,0,1,0],[0,r,0,1]])\n",
    "\n",
    "E_1x_1y_2x_2y = np.random.multivariate_normal(mean=mean, cov=cov,size=nsamples)\n",
    "print(E_1x_1y_2x_2y.shape)\n",
    "\n",
    "# fake data set (acentric)\n",
    "ds_test =         pd.DataFrame(np.sqrt(E_1x_1y_2x_2y[:,0]**2 +E_1x_1y_2x_2y[:,1]**2 ), columns = [\"EP_1\"])\n",
    "ds_test[\"EP_2\"] = pd.DataFrame(np.sqrt(E_1x_1y_2x_2y[:,2]**2 +E_1x_1y_2x_2y[:,3]**2 ))\n",
    "\n",
    "# graphical inspection\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(ds_test[\"EP_1\"].to_numpy(),100,density=True)\n",
    "plt.plot(x, wilson_dist_normalized(x, centric=False),'r-')\n",
    "plt.plot(x, rice.pdf(x,0,scale=np.sqrt(0.5)),'m--')\n",
    "plt.legend([\"Acentric Wilson\", r\"Rice($\\nu=0$,$\\sigma^2=\\frac{1}{2}$) \", \"Fake observed E (acentric)\"])\n",
    "plt.xlabel(\"Normalized structure factor amplitude\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE DATA generation & graphical inspection (centric)\n",
    "Now the same inspection for centric reflections. Note the subtle factor 2 resulting from multiplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data set (centric)\n",
    "# mind the sqrt(2)! we normalize, in effect, by the rms acentric amplitude\n",
    "ds_test_cent = pd.DataFrame(        np.sqrt(2*E_1x_1y_2x_2y[:,0]**2), columns = [\"EP_1\"])\n",
    "ds_test_cent[\"EP_2\"] = pd.DataFrame(np.sqrt(2*E_1x_1y_2x_2y[:,2]**2))\n",
    "\n",
    "print(ds_test_cent.mean())\n",
    "# graphical inspection\n",
    "plt.figure()\n",
    "plt.hist(ds_test_cent[\"EP_2\"].to_numpy(),100,density=True)\n",
    "plt.plot(x, wilson_dist_normalized(x, centric=True),'r-')\n",
    "plt.legend([\"Centric Wilson\", \"Fake observed E (centric)\"])\n",
    "plt.xlabel(\"Normalized structure factor amplitude\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE DATA: Conditional distribution for Acentrics. Step 1: means per bin for $|E_1|$.\n",
    "We will calculate the conditional distributions of $|E_2|$ per bin of $|E_1|$. First, for **acentrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cdf_bins = np.linspace(0,100,nbin+1)\n",
    "ep1_bins = np.percentile(ds_test[\"EP_1\"], cdf_bins)\n",
    "ep1_bins[-1] = ep1_bins[-1]+1e-6 # to avoid omitting the largest data point\n",
    "ep1_dig  = np.digitize(ds_test[\"EP_1\"], ep1_bins)\n",
    "ds_test[\"EP_1_bin\"] = ep1_dig.flatten()\n",
    "\n",
    "unique, counts = np.unique(ds_test[\"EP_1_bin\"], return_counts=True)\n",
    "temp = dict(zip(unique, counts))\n",
    "print(\"Number of reflections per bin:\")\n",
    "print(temp)\n",
    "\n",
    "bin_means=ds_test.groupby(\"EP_1_bin\")[\"EP_1\"].mean()\n",
    "plt.plot(bin_means,'bo-')\n",
    "plt.xlabel(\"Bin index\")\n",
    "plt.ylabel(\"Mean |E1| for bin\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE DATA: Conditional distribution for Acentrics. Step 2: histograms of $|E_2|$ per bin of $|E_1|$.\n",
    "The calculations of the parameters for the Rice distribution are not very intuitive. \n",
    "\n",
    "The conditional mean     $\\mathbb{E}\\left(E_2 | E_1\\right) = r_{DW} E_1$ (centric and acentric; true for amplitudes and complex structure factors).\n",
    "\n",
    "The conditional variance $\\textrm{Var}\\left(|E_2| | |E_1| \\right) = \\frac{1}{2}\\left(1-r_{DW}^2\\right)$ (acentric)\n",
    "or                                                                           $\\left(1-r_{DW}^2\\right)$ (centric),\n",
    "where the twice-as-large conditional variance for centrics is inherent in how they are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))   # for illustration purposes use 15, 5\n",
    "x=np.linspace(0,3,100)\n",
    "counter = 0\n",
    "for i in range(1,nbin+1):     # for illustration purposes use [1, 13, 25] \n",
    "    plt.subplot(5,5,i)\n",
    "#    counter += 1             # for illustration purposes.\n",
    "#    plt.subplot(1,3,counter) # for illustration purposes.\n",
    "    rho = r_fake                   # see above\n",
    "    r1 = rho*bin_means[i]     # conditional exp value\n",
    "    cond_var = 0.5*(1-rho**2) # conditional var for acentrics\n",
    "#    \n",
    "    plt.hist(ds_test.loc[ds_test[\"EP_1_bin\"]==i,\"EP_2\"],np.linspace(0,5,50),density=True)\n",
    "    plt.plot(x, rice.pdf(x, r1/np.sqrt(cond_var),scale=np.sqrt(cond_var)),'r-')\n",
    "    yl=plt.ylim()\n",
    "#\n",
    "#    to illustrate the idea of Rice interpolating between Wilson and delta function:\n",
    "#    plt.plot(bin_means[i]*np.asarray([1, 1]),yl,'k-')\n",
    "#    plt.plot(rice.mean(0,scale=1/np.sqrt(2))*np.asarray([1,1]),yl,'b-')\n",
    "#    plt.plot(rice.mean(r1/np.sqrt(cond_var),scale=np.sqrt(cond_var))*np.asarray([1,1]),yl,'r:')\n",
    "    xl=[0,3] #plt.xlim()\n",
    "    plt.text(1.5,0.85*yl[1],f\"<E1> = {bin_means[i]:.2f}\",size=10)\n",
    "    plt.xlim(xl)\n",
    " \n",
    "plt.subplot(5,5,3)\n",
    "plt.title(\"Conditional histograms (acentric, fake data)\")\n",
    "plt.show()\n",
    "# Basic check:\n",
    "#    plt.plot(np.sqrt((1-rho**2)+rho**2 * bin_means[i]**2), np.sqrt(np.mean(ds_test[\"EP_2\"][ds_test[\"EP_1_bin\"]==i]**2)), 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE DATA: Conditional distribution for Centrics. Step 1: means per bin for $|E_1|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cdf_cent_bins     = np.linspace(0,100,nbin+1)\n",
    "ep1_cent_bins     = np.percentile(ds_test_cent[\"EP_1\"], cdf_cent_bins)\n",
    "ep1_cent_bins[-1] = ep1_cent_bins[-1]+1e-6 # to avoid omitting the largest data point\n",
    "ep1_cent_dig  = np.digitize(ds_test_cent[\"EP_1\"], ep1_cent_bins)\n",
    "ds_test_cent[\"EP_1_bin\"] = ep1_cent_dig.flatten()\n",
    "\n",
    "unique, counts = np.unique(ds_test_cent[\"EP_1_bin\"], return_counts=True)\n",
    "temp = dict(zip(unique, counts))\n",
    "print(\"Number of reflections per bin:\")\n",
    "print(temp)\n",
    "\n",
    "bin_means_cent=ds_test_cent.groupby(\"EP_1_bin\")[\"EP_1\"].mean()\n",
    "plt.plot(bin_means_cent,'bo-')\n",
    "plt.xlabel(\"Bin index\")\n",
    "plt.ylabel(\"Mean |E1| for bin\")\n",
    "plt.title(\"Centric\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAKE DATA: Conditional distribution for Centrics. Step 2: histograms of $|E_2|$ per bin of $|E_1|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "x=np.linspace(0,3,100)\n",
    "for i in range(1,nbin+1):\n",
    "    #print(i)\n",
    "    plt.subplot(5,5,i)\n",
    "    rho = r_fake\n",
    "    r1 = rho*bin_means_cent[i] # conditional exp value\n",
    "    cond_var = (1-rho**2)      # no 0.5* for centrics!\n",
    "    plt.hist(ds_test_cent[\"EP_2\"][ds_test_cent[\"EP_1_bin\"]==i],np.linspace(0,5,50),density=True)\n",
    "    plt.plot(x, foldnorm.pdf(x, r1/np.sqrt(cond_var),scale=np.sqrt(cond_var)),'r-')\n",
    "    xl=[0,3] #plt.xlim()\n",
    "    #yl=plt.ylim()\n",
    "    plt.xlim(xl)\n",
    "    #plt.ylim(yl)\n",
    "    #plt.xlabel(\"Expected conditional mean for parallel component\")\n",
    "    #plt.ylabel(\"Observed conditional mean for parallel component\")\n",
    "\n",
    "plt.subplot(5,5,3)\n",
    "plt.title(\"Conditional histograms (Centric, fake data)\")\n",
    "plt.show()\n",
    "# Basic check:\n",
    "#    plt.plot(np.sqrt((1-rho**2)+rho**2 * bin_means[i]**2), np.sqrt(np.mean(ds_test[\"EP_2\"][ds_test[\"EP_1_bin\"]==i]**2)), 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected correlation coefficients under experimental errors\n",
    "Note: currently this redefines some variables from fake to real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the distribution of dHKL and E in the real data\n",
    "nbin=(20,10)\n",
    "\n",
    "cdf_bins     = np.linspace(0,100,nbin[0]+1)\n",
    "d_bins       = np.percentile(ds1_2[\"dHKL\"], cdf_bins)\n",
    "d_bins[-1]   = d_bins[-1]+1e-6   # to avoid omitting the largest data point\n",
    "d_dig        = np.digitize(ds1_2[\"dHKL\"], d_bins)\n",
    "ds1_2[\"dHKL_bin_test\"] = d_dig.flatten()\n",
    "# print(d_bins)\n",
    "\n",
    "cdf_bins     = np.linspace(0,100,nbin[1]+1)\n",
    "ep1_bins     = np.percentile(ds1_2[\"EP_1\"], cdf_bins)\n",
    "ep1_bins[-1] = ep1_bins[-1]+1e-6 # to avoid omitting the largest data point\n",
    "ep1_dig      = np.digitize(ds1_2[\"EP_1\"], ep1_bins)\n",
    "ds1_2[\"EP_1_bin_test\"] = ep1_dig.flatten()\n",
    "\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(ds1_2[\"dHKL_bin_test\"], ds1_2[\"EP_1_bin_test\"],bins=nbin)\n",
    "# print(H.shape)\n",
    "# print(H)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "im=plt.imshow(H.transpose(), interpolation='nearest', origin='lower',\\\n",
    "           extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],vmin=0)\n",
    "plt.xlabel(\"resolution bin\")\n",
    "plt.ylabel(\"|E| bin\")\n",
    "plt.title(\"Counts per 2D bin for |E| and dHKL\")\n",
    "plt.colorbar(im,fraction=0.025, pad=0.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most columns of the above are fairly uniform, except we find that small-|E| reflections are depleted at high resolution. perhaps this is an artefact of French-Wilson scaling of very weak reflections at high resolution (small $d_{hkl}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approximate the effect of measurement error on our fake data generated above, we'll resample the measurement errors of EP1 of our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample experimental standard deviations\n",
    "count_max = 10*np.max(H[:]).astype(int)\n",
    "print(count_max)\n",
    "sigE_samples = np.zeros([nbin[0],nbin[1],count_max])\n",
    "replace = True    # sample with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "\n",
    "for i in range(nbin[0]):      # resolution\n",
    "    for j in range(nbin[1]):  # E bin\n",
    "        sigE_samples[i][j] = ds1_2[\"SIGEP_1\"][(ds1_2[\"dHKL_bin_test\"]==i+1)&(ds1_2[\"EP_1_bin_test\"]==j+1)].sample(count_max,replace=replace).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an array for fake data with experimental standard deviations and to-be-imposed covariance structure\n",
    "ds1_2_fake=ds1_2[[\"dHKL_bin_test\",\"dHKL\",\"CENTRIC\"]].copy()\n",
    "ds1_2_fake.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient for simulated data with a single global correlation coefficient drops off much more slowly than for the observations. This seems to imply two things: \n",
    "\n",
    "1. Measurement errors are wildly overestimated at low resolution, and perhaps underestimated at high resolution,\n",
    "AND/OR\n",
    "2. The correlation coefficient should really be considered resolution-dependen\n",
    "\n",
    "The latter is far more plausible. Indeed, R. Read, 1990 \"Structure-Factor Probabilities for Related Structures\" confirms this perspective and suggests a form $\\rho=a e^{-b s^s}$. Based on that paper, **in case of isotropic, normally distributed coordinate shifts**, $b\\sim \\frac{2\\pi^2}{3} \\left< |\\Delta r_j|^2\\right>$. For example, $b=1$ corresponds to a root mean square coordinate shift (in 3D) of $\\sqrt{\\left< |\\Delta r_j|^2\\right>} =\\frac{1}{\\pi} \\sqrt{3/2}$ = 0.4 A. More generally, the assumed rms coordinate shift is $0.39 \\sqrt{b}$ in Angstrom. In this model, $a=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement resolution-dependent sampling of E2 given E1 for fake data structured like the real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acentric = (ds1_2_fake[\"CENTRIC\"].to_numpy() == False)\n",
    "centric  = (ds1_2_fake[\"CENTRIC\"].to_numpy() == True )\n",
    "count_by_bin_ac = ds1_2_fake[\"dHKL\"][acentric].groupby(ds1_2_fake[\"dHKL_bin_test\"]).count().to_numpy()\n",
    "res_means_ac    = ds1_2_fake[\"dHKL\"][acentric].groupby(ds1_2_fake[\"dHKL_bin_test\"]).mean( ).to_numpy()\n",
    "count_by_bin_c  = ds1_2_fake[\"dHKL\"][centric ].groupby(ds1_2_fake[\"dHKL_bin_test\"]).count().to_numpy()\n",
    "res_means_c     = ds1_2_fake[\"dHKL\"][centric ].groupby(ds1_2_fake[\"dHKL_bin_test\"]).mean( ).to_numpy()\n",
    "print(count_by_bin_ac)\n",
    "print(count_by_bin_c)\n",
    "\n",
    "a=0.98                   # per Read, 1990, one expects a=1 for isotropic coordinate changes drawn from a normal dist\n",
    "b=1.2\n",
    "mean    = [0,0,0,0]            # mean of random walks\n",
    "ds1_2_fake[\"EP_1\"] = np.zeros(ds1_2_fake.shape[0])\n",
    "ds1_2_fake[\"EP_2\"] = np.zeros(ds1_2_fake.shape[0])\n",
    "for i in range(nbin[0]):       # loop over resolution bins\n",
    "    # let's generate acentric pairs (the assignments to parts of columns create warnings)\n",
    "    r = a*np.exp(-b/(res_means_ac[i]**2))\n",
    "#     print(\"bin \" + str(i+1) + \": \" + str(r) + \" for avg res in bin: \" + str(res_means_ac[i]) + \" (acentric)\")\n",
    "#   print(count_by_bin_ac[i])\n",
    "    cov           = 0.5*np.asarray([[1,0,r,0],[0,1,0,r],[r,0,1,0],[0,r,0,1]])\n",
    "    E_1x_1y_2x_2y = np.random.multivariate_normal(mean=mean, cov=cov,size=count_by_bin_ac[i])\n",
    "    \n",
    "    # the following assignment approach generates some warnings!\n",
    "    ds1_2_fake.loc[(ds1_2_fake[\"dHKL_bin_test\"]==i+1) & acentric, \"EP_1\"] = np.sqrt(E_1x_1y_2x_2y[:,0]**2 +E_1x_1y_2x_2y[:,1]**2 )\n",
    "    ds1_2_fake.loc[(ds1_2_fake[\"dHKL_bin_test\"]==i+1) & acentric, \"EP_2\"] = np.sqrt(E_1x_1y_2x_2y[:,2]**2 +E_1x_1y_2x_2y[:,3]**2 )\n",
    "    \n",
    "    # now for centric pairs\n",
    "    r = a*np.exp(-b/(res_means_c[i]**2))\n",
    "#     print(\"bin \" + str(i+1) + \": \" + str(r) + \" for avg res in bin: \" + str(res_means_c[i]) + \" (centric)\")\n",
    "    cov           = 0.5*np.asarray([[1,0,r,0],[0,1,0,r],[r,0,1,0],[0,r,0,1]])\n",
    "    E_1x_1y_2x_2y = np.random.multivariate_normal(mean=mean, cov=cov,size=count_by_bin_c[i])\n",
    "    \n",
    "    # the following assignment approach generates some warnings!\n",
    "    ds1_2_fake.loc[(ds1_2_fake[\"dHKL_bin_test\"]==i+1) & centric,\"EP_1\"] = np.sqrt(2*E_1x_1y_2x_2y[:,0]**2)\n",
    "    ds1_2_fake.loc[(ds1_2_fake[\"dHKL_bin_test\"]==i+1) & centric,\"EP_2\"] = np.sqrt(2*E_1x_1y_2x_2y[:,2]**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can probably be vectorized, or the final assignment can be done in one step.\n",
    "# we're assuming that the same error distribution applies to normalized centric and acentric structure factors.\n",
    "# from tqdm import tqdm\n",
    "\n",
    "ep1_fake_bins = ep1_bins.copy()   # these bins are based on real data. \n",
    "ep1_fake_bins[-1] = 100\n",
    "ep2_fake_bins = ep1_bins.copy()   # the same as for EP1\n",
    "ep2_fake_bins[-1] = 100\n",
    "\n",
    "ep1_fake_dig  = np.digitize(ds1_2_fake[\"EP_1\"], ep1_fake_bins)\n",
    "ep2_fake_dig  = np.digitize(ds1_2_fake[\"EP_2\"], ep2_fake_bins)\n",
    "# print(plt.hist(ep2_fake_dig)) # these bins are not great for fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an array of fake measurements with the same EP1 and dHKL bins as the data\n",
    "S = ds1_2_fake.shape[0]\n",
    "ep1_fake_obs    = np.zeros((S,))\n",
    "sig_ep1_fake_obs= np.zeros((S,))\n",
    "count_array     = np.zeros(nbin).astype(int)\n",
    "for i in tqdm(range(S)):\n",
    "    i_E    = ep1_fake_dig[i] - 1\n",
    "    i_dHKL = ds1_2_fake.iloc[i][\"dHKL_bin_test\"] - 1\n",
    "    i_dHKL = i_dHKL.astype(int)                       \n",
    "    sigma  = sigE_samples[i_dHKL][i_E][count_array[i_dHKL][i_E]]\n",
    "    loc    = ds1_2_fake.iloc[i][\"EP_1\"]\n",
    "    count_array[i_dHKL][i_E] += 1\n",
    "    ep1_fake_obs[i]           = np.random.normal(loc=loc,scale=sigma)\n",
    "    sig_ep1_fake_obs[i]       = sigma\n",
    "    \n",
    "ds1_2_fake[\"EP_1_obs\"   ] = ep1_fake_obs\n",
    "ds1_2_fake[\"SIGEP_1_obs\"] = sig_ep1_fake_obs\n",
    "\n",
    "# same for the other one, we continue counting in the counting array\n",
    "# i.e. uncorrelated errors (errors may well be correlated in reality!)\n",
    "ep2_fake_obs= np.zeros((S,))\n",
    "sig_ep2_fake_obs= np.zeros((S,))\n",
    "for i in tqdm(range(S)):\n",
    "    i_E    = ep2_fake_dig[i] - 1\n",
    "    i_dHKL = ds1_2_fake.iloc[i][\"dHKL_bin_test\"] - 1\n",
    "    i_dHKL = i_dHKL.astype(int)                       # for some reason, the last one or two are corrupted\n",
    "    sigma  = sigE_samples[i_dHKL][i_E][count_array[i_dHKL][i_E]]\n",
    "    loc    = ds1_2_fake.iloc[i][\"EP_2\"]\n",
    "    count_array[i_dHKL][i_E] += 1\n",
    "    ep2_fake_obs[i]           = np.random.normal(loc=loc,scale=sigma)\n",
    "    sig_ep2_fake_obs[i]       = sigma\n",
    "    \n",
    "ds1_2_fake[\"EP_2_obs\"]=ep2_fake_obs\n",
    "ds1_2_fake[\"SIGEP_2_obs\"]=sig_ep2_fake_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these \"observed\" (i.e. with measurement error) fake data. The way this is calculated, they can be a little negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds1_2_fake[\"EP_1_obs\"], ds1_2_fake[\"EP_2_obs\"],'bo',alpha=0.015)\n",
    "plt.xlabel(\"E1 (fake observed)\")\n",
    "plt.ylabel(\"E2 (fake observed)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_2_fake.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to relate the expected correlation coefficient under the double-Wilson model to the expected correlation coefficient when we add in model error. The underlying model is as follows: For each $hkl$, we'll consider that for the first model, the true structure factor amplitudes are $EP1_{true}=x$ and $EP2_{true}=x+\\epsilon$. That is, $\\epsilon$ captures fixed effects. Instead, we observe $EP1_{obs}=x+\\eta_1$ and $EP2_{obs}=x+\\epsilon + \\eta_2$ with $\\eta_1$ and $\\eta_2$ the measurement errors. It does not matter whether the errors are truely additive--these relationships can be considered to define $\\eta_1$ and $\\eta_2$. We will assume that the reported errors are accurate estimates of the standard deviations of  $\\eta_1$ and $\\eta_2$. \n",
    "\n",
    "Then,\n",
    "\n",
    "$\\rho_{obs} = \\rho(x+\\eta_1,x+\\epsilon+\\eta_2)=\\frac{\\sigma_x^2}{\\sqrt{\\left(\\sigma_x^2 + \\sigma_1^2 \\right)                  \n",
    "                                                                       \\left(\\sigma_x^2 + \\sigma_\\epsilon^2\n",
    "                                                                                        + \\sigma_2^2\\right)}\n",
    "                                                                       }$ \n",
    "and \n",
    "$\\rho_{true}= \\rho(x,x+\\epsilon)=\\frac{\\sigma_x^2}{\\sqrt{\\sigma_x^2 \\left(\\sigma_x^2+\\sigma_\\epsilon^2\\right)}}$\n",
    "\n",
    "where we've used that all covariances between $x,\\epsilon,\\eta_1,\\eta_2$ are 0 (except \"self-covariance\") and abbreviated error std's as $\\sigma_1$ and $\\sigma_2$. Some algebra yields:\n",
    "\n",
    "$\\rho_{obs}^{-2} = \\rho_{true}^{-2} + \\frac{\\sigma^2_1+\\sigma^2_2}{\\sigma^2_x} + \\frac{\\sigma^2_1 \\sigma^2_2}{\\sigma^4_x} + \\frac{\\sigma^2_1}{\\sigma^2_x}\\left(\\rho_{true}^{-2}-1 \\right)$\n",
    "\n",
    "Since it is arbitrary to assign the systematic effects to dataset 2 rather than dataset 1, we symmetrize the last term below to $\\frac{\\sigma_1\\sigma_2}{\\sigma^2_x}\\left(\\rho_{true}^{-2}-1 \\right)$.\n",
    "\n",
    "Note that $\\sigma^2_x$ is, in our model, the variance of the Wilson distribution for normalized structure factors. This variance differs between centric and acentric wilson distributions. For simplicity, I use below estimates obtained above by sampling. Because the estimated $\\rho_{obs}$ depends on error estimates of structure factor amplitudes, we estimate it below *for each observed normalized structure factor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a: \" + str(a))\n",
    "print(\"b: \" + str(b))\n",
    "d          = ds1_2_fake[\"dHKL\"].to_numpy()\n",
    "r_by_res   = a*np.exp(-b/(d**2))                  # DW r\n",
    "r_by_res   = r_by_res.flatten()\n",
    "rho_no_err = r_dw_to_r_E(r_by_res,beta_ac)        # rho from DW r; treat all as acentric (small error); almost rho=r**2\n",
    "rho_no_err = rho_no_err.flatten()\n",
    "\n",
    "varW_acentric= 0.215  # by sampling from above; replace with analytical\n",
    "varW_centric = 0.362  # by sampling from above\n",
    "tmp1        = ds1_2_fake[\"SIGEP_1_obs\"].to_numpy()**2 # \n",
    "tmp2        = ds1_2_fake[\"SIGEP_2_obs\"].to_numpy()**2 # \n",
    "quad_var_E  = tmp1+tmp2\n",
    "\n",
    "rho_w_err_inv2_ac = (1/rho_no_err**2) + quad_var_E/varW_acentric +\\\n",
    "                   tmp1*tmp2/varW_acentric**2 + ((1/rho_no_err**2)-1)*np.sqrt(tmp1*tmp2)/varW_acentric\n",
    "rho_w_err_inv2_c  = (1/rho_no_err**2) + quad_var_E/varW_centric +\\\n",
    "                   tmp1*tmp2/varW_centric**2  + ((1/rho_no_err**2)-1)*np.sqrt(tmp1*tmp2)/varW_centric\n",
    "\n",
    "rho_w_err_ac = 1/np.sqrt(rho_w_err_inv2_ac) #(rho_no_err**2/(1+rho_no_err**2  + (tmp**2)/(varW_acentric)))\n",
    "rho_w_err_c  = 1/np.sqrt(rho_w_err_inv2_c)  #(rho_no_err**2/(1+rho_no_err**2  + (tmp**2)/(varW_acentric)))\n",
    "\n",
    "ds1_2_fake[\"Exp rho w error\"]=np.nan\n",
    "ds1_2_fake.loc[acentric,\"Exp rho w error\"] = rho_w_err_ac[acentric]\n",
    "ds1_2_fake.loc[centric, \"Exp rho w error\"] = rho_w_err_c[  centric]\n",
    "\n",
    "plt.plot(1/d**2, rho_no_err, 'r.',alpha=0.01)\n",
    "plt.plot(1/d**2, rho_w_err_ac, 'b.',alpha=0.01)\n",
    "plt.title('Expected Pearson r(E1,E2) with & w.o. expt error')\n",
    "plt.xlabel(r\"$1/d^2$ ($A^{-2}$)\")\n",
    "plt.ylabel(\"Expected Pearson r(E1,E2)\")\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison, we estimate pre resolution bin an effective expected average correlation coefficient under the model with experimental errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds1_2_fake # view for brevity\n",
    "rho_avg=np.zeros((20,1))\n",
    "for i in range(ds[\"dHKL_bin_test\"].min(),ds[\"dHKL_bin_test\"].max()+1):\n",
    "    boi = (ds[\"dHKL_bin_test\"]==i)\n",
    "    rho = ds[\"Exp rho w error\"][boi].to_numpy()\n",
    "    w   = (ds[\"SIGEP_1_obs\"][boi]*ds[\"SIGEP_2_obs\"][boi]).to_numpy()\n",
    "    tmp = np.sum(w*rho)/np.sum(w)\n",
    "    rho_avg[i-1] = np.sum(w*rho)/np.sum(w) # np.mean(rho) gives nearly the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've guessed *a and b*. We'll now put all the calculations above together into a fitting routine. For simplicity, we approximate the correlation between error-less normalized structure factor amplitudes under the model as $\\rho_{DW}=r^2_{DW}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ab_ds_residual(p,ds,labels=[\"EP_1\",\"EP_2\"],dHKL_bin_label=\"dHKL_bin_test\"):\n",
    "    print(p)\n",
    "    nbin = ds[dHKL_bin_label].nunique()\n",
    "    \n",
    "    s     = 1/ds[\"dHKL\"].to_numpy()\n",
    "    r_DW  = p[0]*np.exp(-p[1]*s**2)\n",
    "    rho_DW= r_DW**2           # this is approximate as described above\n",
    "    \n",
    "    varW_acentric= 0.215      # by sampling above; replace with analytical\n",
    "    varW_centric = 0.362      # by sampling above\n",
    "    tmp1        = ds[\"SIG\"+labels[0]].to_numpy()**2 # sigE's\n",
    "    tmp2        = ds[\"SIG\"+labels[1]].to_numpy()**2 # \n",
    "    quad_var_E  = tmp1+tmp2\n",
    "\n",
    "    rho_w_err_inv2_ac = (1/rho_DW**2) + quad_var_E/varW_acentric +\\\n",
    "                       tmp1*tmp2/varW_acentric**2 + ((1/rho_DW**2)-1)*np.sqrt(tmp1*tmp2)/varW_acentric\n",
    "    rho_w_err_inv2_c  = (1/rho_DW**2) + quad_var_E/varW_centric +\\\n",
    "                       tmp1*tmp2/varW_centric**2  + ((1/rho_DW**2)-1)*np.sqrt(tmp1*tmp2)/varW_centric\n",
    "\n",
    "    rho_w_err_ac = 1/np.sqrt(rho_w_err_inv2_ac)\n",
    "    rho_w_err_c  = 1/np.sqrt(rho_w_err_inv2_c ) \n",
    "    centric = (ds[\"CENTRIC\"]==True).to_numpy()\n",
    "    ds.loc[ds[ \"CENTRIC\"], \"rho_w_err\"] = rho_w_err_c[  centric]\n",
    "    ds.loc[~ds[\"CENTRIC\"], \"rho_w_err\"] = rho_w_err_ac[~centric]\n",
    "    \n",
    "    # empirical correlation by bin\n",
    "    g = ds.groupby(dHKL_bin_label)[labels]\n",
    "    result = g.corr(method=\"pearson\").unstack().loc[:, (labels[0],labels[1])].to_numpy().flatten()\n",
    "    \n",
    "    # error-weighted avg predicted rho (this way of weighting comes from the definition of Pearson r \n",
    "    #                                   and sample cov being a sum over samples)\n",
    "    # in what I've seen so far, weighting has a minimal effect.\n",
    "    rho_avg=np.zeros((nbin,1))\n",
    "    for i in range(ds[dHKL_bin_label].min(),ds[dHKL_bin_label].max()+1):\n",
    "        boi     = (ds[dHKL_bin_label]==i)\n",
    "        rho     =  ds[\"rho_w_err\"][boi].to_numpy()\n",
    "        w       = (ds[\"SIG\" + labels[0]][boi]*ds[\"SIG\" + labels[1]][boi]).to_numpy()\n",
    "        rho_avg[i-1] = np.sum(w*rho)/np.sum(w) \n",
    "    \n",
    "    rho_avg=rho_avg.flatten()\n",
    "\n",
    "    return (result-rho_avg)\n",
    "\n",
    "#a is constrained to be in [0,1], b to be in [0,1e6] (frivolous upper bound)\n",
    "fit_a_b = least_squares(fit_ab_ds_residual, [0.5,1], args=(ds1_2,[\"EP_1\",\"EP_2\"],\"dHKL_bin_test\"),\\\n",
    "                             verbose=1,bounds=[(0,0),(1,1e6)],diff_step=0.01) # diff_step is important for this not to get stuck earlier\n",
    "a=fit_a_b.x[0]\n",
    "b=fit_a_b.x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the data to our double-wilson model with resolution-dependent $r_{DW}$, either using the sampling procedure described first, or the (almost) analytical fitting procedure described right above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1, labels_1 = compute_cc(ds1_2_fake,labels=[\"EP_1_obs\",\"EP_2_obs\"],nbins=20,method=\"pearson\")\n",
    "result_3, labels_3 = compute_cc(ds1_2,     labels=[\"EP_1\",    \"EP_2\"    ],nbins=20,method=\"pearson\")\n",
    "\n",
    "# we'll reverse-engineer Jack's labels to add the expected curve in the absence of expt error\n",
    "res_range = np.zeros([20,1])\n",
    "for i in range(len(labels_1)):\n",
    "    tmp1 = float(labels_1[i].split()[0])\n",
    "    tmp3 = float(labels_1[i].split()[2])\n",
    "    tmp2 = np.sqrt(tmp1*tmp3) # geom mean\n",
    "    res_range[i] = tmp2\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(20), a*np.exp(-b/(res_range**2)),'r--')\n",
    "plt.plot(np.arange(20), r_dw_to_r_E(a*np.exp(-b/(res_range**2)),beta_ac).flatten(),'m--') # this is correct for acentrics\n",
    "plt.plot(np.flip(np.arange(20)), rho_avg, 'k-',linewidth=2)\n",
    "plot_by_res_bin(result_1, labels_1)\n",
    "plot_by_res_bin(result_3, labels_3,color='g')\n",
    "plt.legend([\"DW r\", \"DW rho(E1,E2)\", \"DW rho_obs(E1,E2) (w. avg)\", \"DW sampling expt err\", \"observed\"])\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"CC (EP1, EP2)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the correlation coefficient in this data set is dominated by the true differences in structures (magenta), with small contributions from experimental error (observed: green; fake data, resampled: blue; model estimate using experimental errors: black)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the same steps for real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REAL DATA: Means per bin for $|E_1|$, determined separately for acentrics and centrics.\n",
    "We will calculate the conditional distributions of $|E_2|$ per bin of $|E_1|$. We'll set bin labels for both centrics and acentrics here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbin=25\n",
    "cdf_bins = np.linspace(0,100,nbin+1)\n",
    "\n",
    "ds1_2_centric  = (ds1_2[\"CENTRIC\"].to_numpy()==True)\n",
    "ds1_2_acentric = (ds1_2[\"CENTRIC\"].to_numpy()==False)\n",
    "\n",
    "ep1_bins          = np.percentile(ds1_2[\"EP_1\"][ds1_2_acentric], cdf_bins)\n",
    "ep1_bins[-1]      = ep1_bins[-1]+1e-6                # to avoid omitting the largest data point\n",
    "ep1_dig           = np.digitize(ds1_2[\"EP_1\"][  ds1_2_acentric], ep1_bins) # note that we're creating bin labels for all reflections, acentric or not\n",
    "\n",
    "ep1_cent_bins     = np.percentile(ds1_2[\"EP_1\"][ds1_2_centric], cdf_bins)\n",
    "ep1_cent_bins[-1] = ep1_cent_bins[-1]+1e-6\n",
    "ep1_cent_dig      = np.digitize(ds1_2[\"EP_1\"][  ds1_2_centric], ep1_cent_bins) # note that we're creating bin labels for all reflections, acentric or not\n",
    "\n",
    "ds1_2.loc[ds1_2_acentric, \"EP_1_bin\"] = ep1_dig.flatten()\n",
    "ds1_2.loc[ds1_2_centric,  \"EP_1_bin\"] = ep1_cent_dig.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some inspection of the created bins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For acentrics:\n",
    "unique, counts = np.unique(ds1_2[\"EP_1_bin\"][ds1_2_acentric], return_counts=True)\n",
    "temp = dict(zip(unique, counts))\n",
    "print(\"Number of reflections per bin (acentric):\")\n",
    "print(temp)\n",
    "print()\n",
    "\n",
    "# for centrics\n",
    "unique, counts = np.unique(ds1_2[\"EP_1_bin\"][ds1_2_centric], return_counts=True)\n",
    "temp = dict(zip(unique, counts))\n",
    "print(\"Number of reflections per bin (centric):\")\n",
    "print(temp)\n",
    "\n",
    "bin_means_acentric = ds1_2[ds1_2_acentric].groupby(\"EP_1_bin\")[\"EP_1\"].mean()\n",
    "bin_means_centric  = ds1_2[ds1_2_centric ].groupby(\"EP_1_bin\")[\"EP_1\"].mean()\n",
    "plt.plot(bin_means_acentric,'bo-')\n",
    "plt.plot(bin_means_centric, 'ro-')\n",
    "plt.xlabel(\"Bin index\")\n",
    "plt.ylabel(\"Mean |E1| for bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REAL DATA: Conditional distribution for Acentrics. Histograms of $|E_2|$ per bin of $|E_1|$.\n",
    "In the following few cells, we will ignore resolution-dependence of $r_{DW}$. That is obviously not entirely accurate, but the work below was done before that was worked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "x=np.linspace(0,3,100)\n",
    "for i in range(1,nbin+1):\n",
    "    plt.subplot(5,5,i)\n",
    "    rho = 0.8                        # a guess of the corr coef between data sets\n",
    "    r1  = rho*bin_means_acentric[i]  # conditional exp value of E1 given E2\n",
    "    cond_var = 0.5*(1-rho**2)        # conditional variance  of E1 given E2\n",
    "#    \n",
    "    plt.hist(ds1_2.loc[(ds1_2[\"EP_1_bin\"]==i) & ds1_2_acentric,\"EP_2\"],np.linspace(0,5,50),density=True)\n",
    "    plt.plot(x, rice.pdf(x, r1/np.sqrt(cond_var), scale=np.sqrt(cond_var)),'r-')\n",
    "    xl=[0,3] \n",
    "    plt.xlim(xl)\n",
    "\n",
    "plt.subplot(5,5,3)\n",
    "plt.title(\"Conditional histograms for E1 given E2 (acentric, real data)\")\n",
    "plt.show()\n",
    "# Basic check:\n",
    "#    plt.plot(np.sqrt((1-rho**2)+rho**2 * bin_means[i]**2), np.sqrt(np.mean(ds_test[\"EP_2\"][ds_test[\"EP_1_bin\"]==i]**2)), 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REAL DATA: Conditional distribution for Centrics. Step 1: means per bin for $|E_1|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "x=np.linspace(0,3,100)\n",
    "for i in range(1,nbin+1):\n",
    "    plt.subplot(5,5,i)\n",
    "    rho = 0.8                       # a guess of the corr coef between data sets\n",
    "    r1  = rho*bin_means_centric[i]   # conditional exp value of E1 given E2\n",
    "    cond_var = (1-rho**2)            # conditional variance  of E1 given E2\n",
    "#    \n",
    "    plt.hist(ds1_2.loc[(ds1_2[\"EP_1_bin\"]==i) & ds1_2_centric,\"EP_2\"],np.linspace(0,5,50),density=True)\n",
    "    plt.plot(x, foldnorm.pdf(x, r1/np.sqrt(cond_var), scale=np.sqrt(cond_var)),'r-')\n",
    "    xl=[0,3] \n",
    "    plt.xlim(xl)\n",
    "\n",
    "plt.subplot(5,5,3)\n",
    "plt.title(\"Conditional histograms for E1 given E2 (centric, real data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating effective Rice dist parameters per reflection\n",
    "Consider two data sets of normalized structure factor amplitudes, $\\{x\\}$ and $\\{x+\\epsilon+\\eta\\}$, with the first data set ideal, $\\epsilon$ the fixed, true differences between the data sets and $\\eta$ the measurement errors. In this case, $\\textrm{Cov}(x,x+\\epsilon)$ is given by the Double-Wilson distribution, as is $\\sigma^2_{x}$. Once we've estimated $a$ and $b$ above such that $r(s)=a\\cdot e^{-b s^2}$, we can calculate what value we should use to constrain a prior distribution for another data set by taking into account that our estimates are tainted by measurement error. \n",
    "\n",
    "To begin, we'll use the approximate relationship $\\rho(|E_1|,|E_2|)=r_{DW}^2$ and the relationship\n",
    "\n",
    "$\\rho^{-2}_{obs}=\\rho^{-2}_{DW} +\\frac{\\sigma^2_{\\eta}}{\\sigma^2_{x}}$,\n",
    "\n",
    "with $\\sigma_x$ known from the Wilson distribution and different between centric and acentric reflections. This equation follows from:\n",
    "\n",
    "$\\rho(x,x+\\varepsilon+\\eta) = \\frac{\\sigma_x}{\\sqrt{\\sigma^2_x+\\sigma^2_{\\epsilon}+\\sigma^2_{\\eta}}}$\n",
    "and $\\rho_{DW}=\\rho(x,x+\\varepsilon)=\\frac{\\sigma_x}{\\sqrt{\\sigma^2_x+\\sigma^2_{\\epsilon}}}$\n",
    "\n",
    "Now,\n",
    "\n",
    "$r_{eff}\\approx\\sqrt{\\rho_{obs}}$   and   $\\rho_{obs}=1/\\sqrt{\\rho^{-2}_{DW} +\\frac{\\sigma^2_{\\eta}}{\\sigma^2_{x}}}$   and   $\\rho_{DW}\\approx a^2 e^{-2bs^s}$ and $s=1/d_{HKL}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eff_r_dw_per_hkl(ds,a,b,label,inplace=True):\n",
    "    s = ds[\"dHKL\"]\n",
    "    rho_DW = (a**2)*np.exp(-2*b/(s**2))           # this is an estimate of the error-free rho(E1,E2)\n",
    "    var_eta = ds[\"SIG\"+label].to_numpy()**2\n",
    "    varW_acentric= 0.215                          # by sampling from above; replace with analytical\n",
    "    varW_centric = 0.362                          # by sampling from above\n",
    "    centric    = (ds[\"CENTRIC\"]==True )\n",
    "    acentric   = (ds[\"CENTRIC\"]==False)\n",
    "    rho_obs_ac = 1/np.sqrt(rho_DW**-2 + var_eta/varW_acentric)\n",
    "    rho_obs_c  = 1/np.sqrt(rho_DW**-2 + var_eta/varW_centric)\n",
    "    print(np.mean(rho_obs_ac))\n",
    "    \n",
    "    r_eff_ac = np.sqrt(rho_obs_ac)\n",
    "    r_eff_c  = np.sqrt(rho_obs_c)\n",
    "    if inplace:\n",
    "        ds.loc[acentric, \"r_DW_out_\"+label] = r_eff_ac[acentric]\n",
    "        ds.loc[ centric, \"r_DW_out_\"+label] = r_eff_c[  centric]\n",
    "        return ds\n",
    "    else:\n",
    "        ds_out=ds.copy()\n",
    "        ds_out.loc[acentric, \"r_DW_out_\"+label] = r_eff_ac[acentric]\n",
    "        ds_out.loc[ centric, \"r_DW_out_\"+label] = r_eff_c[  centric]\n",
    "        return ds_out\n",
    "\n",
    "eff_r_dw_per_hkl(ds1_2,a,b,label=\"EP_1\") # add r_DW column for EP_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at phases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds1_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds1_2[\"PHIC_1\"].to_numpy(),ds1_2[\"PHIC_ALL_1\"].to_numpy(),'bo',alpha=0.02)\n",
    "plt.xlabel(\"PHIC_1 (deg)\")\n",
    "plt.ylabel(\"PHIC_ALL_1 (deg)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not sure what the differences are, but they're all close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the structure of the Von Mises distribution, the conditional probability of the phase difference of E1 and E2 is a function only of E1 x E2. We'll bin by that. For now, we'll only think about acentrics.\n",
    "\n",
    "Specifically, the Von Mises distribution (Bricogne, Methods in Enzymology, eq. 1.10) is given by:\n",
    "\n",
    "$P\\left( \\varphi | R, r \\right) = \\frac{1}{2\\pi I_0(z)}\\exp\\left( z \\cos(\\Delta \\varphi)\\right)$ \n",
    "\n",
    "with $z = \\frac{2 r R}{(1-\\rho^2)}$\n",
    "\n",
    "which can also be derived by dividing eq. 14 for $P(R,\\varphi | r)$ by eq. 15 for $P(R | r)$ in my current notes. The notation is not so appropriate. In this case, $\\rho=r_{DW}$, $R=|E2|$ and $r=|E1|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_2[\"DeltaPHIC\"] = np.remainder((np.pi/180.0)*(ds1_2[\"PHIC_2\"].to_numpy()-ds1_2[\"PHIC_1\"].to_numpy()),2*np.pi)\n",
    "ds1_2[\"cos(DeltaPHIC)\"] = np.cos(ds1_2[\"DeltaPHIC\"])\n",
    "\n",
    "plt.hist(ds1_2[\"DeltaPHIC\"],50 )\n",
    "plt.xlabel(r\"$\\Delta\\phi$\")\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_bins = np.linspace(0,100,nbin+1)\n",
    "\n",
    "ds1_2[\"EP_12\"]  = ds1_2[\"EP_1\"] * ds1_2[\"EP_2\"]                                 \n",
    "\n",
    "# For acentrics:\n",
    "ep12_bins       = np.percentile(ds1_2.loc[ds1_2_acentric, \"EP_12\"], cdf_bins)\n",
    "ep12_bins[-1]   = ep12_bins[-1]+1e-6                # to avoid omitting the largest data point\n",
    "ep12_dig        = np.digitize(ds1_2.loc[  ds1_2_acentric, \"EP_12\"], ep12_bins)\n",
    "ds1_2.loc[ds1_2_acentric, \"EP_12_bin\"] = ep12_dig.flatten()\n",
    "\n",
    "unique, counts = np.unique(ds1_2.loc[ds1_2_acentric, \"EP_12_bin\"], return_counts=True)\n",
    "temp = dict(zip(unique, counts))\n",
    "print(\"Number of reflections per bin (acentric):\")\n",
    "print(temp)\n",
    "\n",
    "bin_means_12_acentric = ds1_2[ds1_2_acentric].groupby(\"EP_12_bin\")[\"EP_12\"].mean()                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for acentrics:\n",
    "plt.figure(figsize=(15,15))\n",
    "x = np.linspace(0,2*np.pi,50)\n",
    "for i in range(1,nbin+1):\n",
    "    plt.subplot(5,5,i)\n",
    "    rho = 0.8                          # a guess of the corr coef between data sets\n",
    "    cond_var = 0.5*(1-rho**2)          # conditional variance  of E1 given E2\n",
    "#    \n",
    "    plt.hist(ds1_2.loc[(ds1_2[\"EP_1_bin\"]==i) & ds1_2_acentric, \"DeltaPHIC\"],np.linspace(0,2*np.pi,50),density=True)\n",
    "    plt.plot(x, vonmises.pdf(x, bin_means_12_acentric[i]/cond_var),'r-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FOLLOWING DOES NOT WORK YET. NOT SURE WHY.\n",
    "# print(ep1_dig.shape)\n",
    "# plt.plot(bin_means_12_acentric/cond_var, ds1_2[\"cos(DeltaPHIC)\"][ds1_2_acentric].groupby(ds1_2[\"EP_12_bin\"]).mean(),'bo')\n",
    "# for i in range(1,15):\n",
    "#     print(i)\n",
    "#     plt.plot(bin_means_12_acentric[i]/cond_var, 1-0.5*vonmises.var(bin_means_12_acentric[i]/cond_var),'rs',alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing reflections to dataset and calculate completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_hkl(cell, sg, res_cutoff):\n",
    "    \"\"\"\n",
    "    set up an empty dataset to a given resolution (mapped to ASU, no systematic absences)\n",
    "    \"\"\"\n",
    "    \n",
    "    max_hkl = np.max([cell.a, cell.b, cell.c])/res_cutoff\n",
    "    hmin, hmax = -max_hkl,max_hkl \n",
    "    H = np.mgrid[hmin:hmax+1:2,hmin:hmax+1:2,hmin:hmax+1:2].reshape((3, -1)).T\n",
    "    ds = rs.DataSet({\"H\": H[:, 0], \"K\": H[:, 1], \"L\": H[:, 2]})\n",
    "    ds.set_index([\"H\", \"K\", \"L\"], inplace=True)\n",
    "    ds.cell=cell\n",
    "    ds.spacegroup=sg                                        ; print(\"entries, initially:       \" + str(ds.shape[0]))\n",
    "    ds_asu = ds.hkl_to_asu(inplace=False)                   \n",
    "    ds_asu = ds_asu[~ds_asu.index.duplicated(keep='first')] ; print(\"entries after map to ASU: \" + str(ds_asu.shape[0]))\n",
    "    ds_asu.label_absences(inplace=True)                      \n",
    "    ds_asu = ds_asu[ds_asu[\"ABSENT\"]==False]                ; print(\"entries wo sys absences:  \" + str(ds_asu.shape[0]))     \n",
    "    ds_asu.compute_dHKL(inplace=True)                       \n",
    "    ds_asu = ds_asu[ds_asu[\"dHKL\"]>=res_cutoff]             ; print(\"entries after res cutoff: \" + str(ds_asu.shape[0]))     \n",
    "    return ds_asu\n",
    "\n",
    "def complete_dataset(ds, res_cutoff=-1, trim_to_res=True):\n",
    "    \"\"\"\n",
    "    Completes a dataset with the missing observations to a given resolution.\n",
    "    \"\"\"\n",
    "    cell=ds.cell\n",
    "    sg  =ds.spacegroup\n",
    "    ds  =ds.compute_dHKL(inplace=True)\n",
    "    if res_cutoff < 0:\n",
    "        res_cutoff = np.min(ds[\"dHKL\"])\n",
    "    if trim_to_res:\n",
    "        ds=ds[ds[\"dHKL\"]>=res_cutoff]\n",
    "    ds_complete = dataset_hkl(cell, sg, res_cutoff)\n",
    "    ds_complete = ds_complete.merge(ds.hkl_to_asu(), how='left',left_index=True,right_index=True,suffixes=[\"\",\"_all\"],indicator=True)#,validate=\"one_to_one\")\n",
    "    ds_complete[\"observed\"]=(ds_complete[\"_merge\"]==\"both\")\n",
    "    return ds_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit slow...\n",
    "ds_complete = complete_dataset(ds1)\n",
    "ds_complete.assign_resolution_bins(inplace=True)\n",
    "ds_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ds_complete[\"dHKL\"].groupby(ds_complete[\"bin\"]).mean(),100*ds_complete[\"observed\"].groupby(ds_complete[\"bin\"]).mean(),'bo-')\n",
    "plt.xlabel(\"Resolution (A)\")\n",
    "plt.ylabel(\"Completeness (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(ds_complete[\"observed\"])\n",
    "# ds_complete = ds_add_rs(ds_complete,force_rs=True)\n",
    "# #print(ds_complete.loc[:,ds_complete[\"observed\"]==False])\n",
    "\n",
    "# fig = plt.figure(figsize=(4,4))\n",
    "# ax = plt.axes(projection=\"3d\")\n",
    "# ax.scatter3D(ds_complete.loc[ds_complete[\"observed\"]==False, \"rs_a\"], \\\n",
    "#              ds_complete.loc[ds_complete[\"observed\"]==False, \"rs_b\"], \\\n",
    "#              ds_complete.loc[ds_complete[\"observed\"]==False, \"rs_c\"],'bo')\n",
    "# ax.view_init(elev=60., azim=120.)\n",
    "# ax.set_xlabel(\"rs_a\")\n",
    "# ax.set_ylabel(\"rs_b\")\n",
    "# ax.set_zlabel(\"rs_c\")\n",
    "# plt.draw()\n",
    "\n",
    "# #for angle in range(0, 360):\n",
    "# #    ax.view_init(30, angle)\n",
    "# #    plt.draw()\n",
    "# #    plt.pause(.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Local scaling\" (BELOW HAS NOT BEEN FINISHED).\n",
    "We'll try to do a version of local scaling by learning a scale factor from reciprocal lattice point coordinates only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sig_cut = 10\n",
    "incl_criteria = (ds1_2[\"FP_1\"]/ds1_2[\"SIGFP_1\"]>sig_cut) & \\\n",
    "                (ds1_2[\"FP_2\"]/ds1_2[\"SIGFP_2\"]>sig_cut)\n",
    "print(\"Number of retained reflections: \" + str(np.sum(incl_criteria.to_numpy())) \\\n",
    "      + \" out of \" + str(incl_criteria.to_numpy().size))\n",
    "X_train, X_test, y_train, y_test = train_test_split(ds1_2[[\"rs_a_1\", \"rs_b_1\", \"rs_c_1\"]][incl_criteria], \n",
    "                                                    ds1_2[\"gamma_pp\"][incl_criteria], \n",
    "                                                    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network (MLP) and set some parameters using a cross-validation grid search. The results for (1BE9 & 1BFE) hit a ceiling that is similar using different parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'alpha':[0.001],'hidden_layer_sizes':[(10,10), (5,5),(3,3,3)],'activation':['identity', 'logistic', 'tanh', 'relu']}\n",
    "nn = MLPRegressor(max_iter=500)\n",
    "reg = GridSearchCV(nn, parameters)\n",
    "reg.fit(X_train, y_train)\n",
    "print(sorted(reg.cv_results_.keys()))\n",
    "#nn = MLPRegressor(\n",
    "#    hidden_layer_sizes=(10,10),  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
    "#    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "#    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "#    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check performance using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reg.best_estimator_)\n",
    "print(reg.best_score_)\n",
    "print(reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "460px",
    "width": "503px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
