{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of individual datasets\n",
    "- takes 2-5 minutes total (up to n=4) or 10-15 min (with n=5)\n",
    "- ironically, the main complication is for datasets which have been FW scaled--noisy reflections are overrepresented near the mean of the Wilson distributions and skew normalization. In the second step of anisotropic scaling I added weights to downweight the effects of noisy reflections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\Doeke Hekstra\\.conda\\envs\\crystallography\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# General tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy   as np\n",
    "import warnings\n",
    "from mpl_toolkits    import mplot3d\n",
    "from scipy.stats     import rice, foldnorm, nakagami, gamma \n",
    "from scipy.optimize  import least_squares, minimize\n",
    "from time            import perf_counter\n",
    "\n",
    "t_overall_start = perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import reciprocalspaceship as rs\n",
    "import gemmi\n",
    "from dw_tools import modify_ds, wilson, plots\n",
    "from dw_tools import aniso_scaling_step_1 as aniso1\n",
    "from dw_tools import aniso_scaling_step_2 as aniso2\n",
    "from dw_tools import knn_tools as knn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors       import KNeighborsRegressor\n",
    "rs.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(300000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 0 out of 48817 records\n"
     ]
    }
   ],
   "source": [
    "# path=\"double_wilson_data\\\\\" # Windows\n",
    "path=\"double_wilson_data/\"    # Unix\n",
    "data_set_pairs = [(\"5kvx_phases\", \"5kw3_phases\"),\\\n",
    "                  (\"5e1y_phases\", \"2vwr_phases\"),\\\n",
    "                  (\"3pyp_phases\", \"1nwz_phases\"),\\\n",
    "                  (\"1otb_phases\", \"1nwz_phases\"),\\\n",
    "                  (\"4eul_phases\", \"GFP_1.37A_scaled\"),\\\n",
    "#                   \"GFP_SSRL_refine_54_final\"),\\\n",
    "                  (\"4eul_phases\", \"2y0g_phases\"),\\\n",
    "                  (\"4kjk_phases\", \"4kjj_phases\", \"4pst_phases\", \"4pss_phases\"),\\\n",
    "                  (\"e35cdef_OFF200ns_HD_2sig_varEll\",\"e35cdef_200ns_HD_2sig_varEll\")]\n",
    "# example 1 fails on kNN regression\n",
    "example=4\n",
    "dataset=1\n",
    "mtz1 = data_set_pairs[example][dataset] # omit .mtz\n",
    "ds1 = rs.read_mtz(path + mtz1 + \".mtz\")\n",
    "\n",
    "# cleanup:\n",
    "unpurged_count = ds1.shape[0];   ds1.dropna(inplace=True)\n",
    "purged_count   = ds1.shape[0]\n",
    "print(f\"Discarded {unpurged_count-purged_count} out of {unpurged_count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'F-obs-filtered'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\crystallography\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'F-obs-filtered'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f4291e4459f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                           \u001b[1;31m# so using naive error propagation instead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mds1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FP\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"F-obs-filtered\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mds1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SIGFP\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SIGF-obs-filtered\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\crystallography\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\crystallography\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'F-obs-filtered'"
     ]
    }
   ],
   "source": [
    "if (example == 4) & (dataset==1): # - GFP_1.37A only contains intensities\n",
    "    I = ds1[\"IMEAN\"].to_numpy()\n",
    "    SIGI = ds1[\"SIGIMEAN\"].to_numpy()\n",
    "    I[I<=0]=0.0001\n",
    "    ds1[\"FP\"] = np.sqrt(I)\n",
    "    ds1[\"SIGFP\"] = 0.5*np.sqrt(SIGI**2/I) #I played with using the Nakagami distribution instead, but it seems that its\n",
    "                                          # std calculation is not very stable\n",
    "                                          # so using naive error propagation instead.\n",
    "# if (example == 4) & (dataset==1):\n",
    "#     ds1[\"FP\"] = ds1[\"F-obs-filtered\"]\n",
    "#     ds1[\"SIGFP\"] = ds1[\"SIGF-obs-filtered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify_ds.merge_anomalous(ds1)\n",
    "# ds1.loc[ds1[\"I(+)\"]>ds1[\"I(-)\"],[\"I(+)\",\"SIGI(+)\",\"I(-)\",\"SIGI(-)\",\"IMEAN\",\"SIGIMEAN\",\"I\",\"SIGI\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, bin_labels = plots.compute_meanF_byres(ds1, label=\"FP\", nbins=20,sigma_cut=0)\n",
    "plots.plot_by_res_bin(result, bin_labels, ylabel=\"mean F\",color='b')\n",
    "plt.title(\"Mean |F|\")\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = ds1[\"FP\"].mean()\n",
    "ds1[\"FP\"] = ds1[\"FP\"]/avg\n",
    "ds1[\"SIGFP\"] = ds1[\"SIGFP\"]/avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "modify_ds.ds_add_rs(ds1)\n",
    "ds1.compute_dHKL(        inplace=True)\n",
    "ds1.label_centrics(      inplace=True)\n",
    "ds1.compute_multiplicity(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking reciprocal space (ASU) coverage\n",
    "Let's see what chunk of reciprocal space these reflections are located in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.scatter3D(ds1[\"rs_a\"], ds1[\"rs_b\"], ds1[\"rs_c\"],alpha=0.01)\n",
    "ax.view_init(elev=60., azim=1200.)\n",
    "ax.set_xlabel(\"rs_a\")\n",
    "ax.set_ylabel(\"rs_b\")\n",
    "ax.set_zlabel(\"rs_c\")\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple anisotropic normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.amin(ds1[\"FP\"].to_numpy()))\n",
    "# print(np.amax(ds1[\"FP\"].to_numpy()))\n",
    "# print(np.sum(np.isnan(ds1[\"FP\"].to_numpy())))\n",
    "res = minimize(         aniso1.anisotropic_scaling_to_1_wilson_loss, [0,0,0,0,0,0,1],\\\n",
    "                                                                method='L-BFGS-B',jac=True,args=(ds1,\"FP\"))\n",
    "# res = minimize(         aniso1.anisotropic_scaling_to_1_wilson_loss, [-5,0,0,-5,0,-5,0.25],\\\n",
    "#                                                                 method='nelder-mead',jac=True,args=(ds1,\"FP\"))\n",
    "res_test, ep1, sigep1 = aniso1.anisotropic_scaling_to_1_wilson_loss(res.x, ds1, label=\"FP\", nargout=3)\n",
    "print(res.x)\n",
    "\n",
    "ds1[\"EP_1_aniso_1\"] = ep1\n",
    "ds1[\"SIGEP_1_aniso_1\"] = sigep1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of histograms. In the absence of a sigma cutoff, we get significant extra counts around the mode of the distribution. This must be the consequence of FW scaling. When we omit these, we see that the histogram is a bit shifted relative the Wilson distribution, suggesting noisy reflections skewed normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_cut = 0.1 # noisy observations deviation significantly from the Wilson distribution!\n",
    "              # ironically, French-Wilson seems to blame--the reflections with largest error \n",
    "              # will tend to get scaled to the mean of the Wilson distribution\n",
    "\n",
    "x=np.linspace(0,4,500)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(ep1[(ds1[\"CENTRIC\"]==True) & (sigep1<sig_cut)],40,alpha=0.5,density=True)\n",
    "plt.plot(x,wilson.wilson_dist_normalized(x,centric=True),'r-') \n",
    "plt.ylabel(\"count\"); plt.xlabel(\"Normalized sf amplitude\")\n",
    "plt.xlim([0,5])\n",
    "plt.title(\"Centric\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(ep1[(ds1[\"CENTRIC\"]==False) & (sigep1<sig_cut)],40,alpha=0.5,density=True)\n",
    "plt.plot(x, wilson.wilson_dist_normalized(x,centric=False),'r-')\n",
    "plt.ylabel(\"count\"); plt.xlabel(\"Normalized sf amplitude\")\n",
    "plt.xlim([0,5])\n",
    "plt.title(\"Acentric\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic normalization with Fourier corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using quite a few parameters, we'll use a test set to choose the optimal order of the Fourier series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rseed=1763\n",
    "np.random.seed(rseed)\n",
    "ds1[\"test_flag\"] = (np.random.random((ds1.shape[0],1)) <0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# typically the minimum is for n=3 or 4...\n",
    "# see right below for typical run times by n (n=4: 2-3 min total)\n",
    "# note that at higher values of n sometimes NaNs pop up\n",
    "\n",
    "res=[0] # start with a dummy so next entries match their n\n",
    "n_best = 0\n",
    "best_loss = 9e9\n",
    "for n in [1,2,3,4,5]:\n",
    "    t1_start = perf_counter()\n",
    "    params = aniso2.prepare_for_FFT(ds1, n=n, label=\"EP_1_aniso_1\")\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        init_guess = np.zeros((1,2*(n**3)))\n",
    "        if n>1:\n",
    "            init_guess[0,:res[-1].x.shape[0]]=res[-1].x\n",
    "        else:\n",
    "            init_guess[0]=np.array([1, 0])\n",
    "        res_tmp = minimize(aniso2.anisotropic_scaling_to_1_FFT_wilson_loss_fast, \\\n",
    "                           init_guess,method='L-BFGS-B', jac = True, \\\n",
    "                           args=(params,ds1[\"test_flag\"].to_numpy()))\n",
    "        res.append(res_tmp)\n",
    "        res_test = aniso2.anisotropic_scaling_to_1_FFT_wilson_loss_fast(res_tmp.x, \\\n",
    "                                          params, bUse=~ds1[\"test_flag\"].to_numpy(), nargout=1)\n",
    "        if res_test[0] < best_loss:\n",
    "            best_loss = res_test[0]\n",
    "            n_best = n\n",
    "        print(f\"For n = {n} the test loss = {res_test[0]:.6}\")\n",
    "        t1_stop = perf_counter()  \n",
    "        print(f\"Elapsed time: {t1_stop-t1_start:.4} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for best n using *all* reflections and save\n",
    "params = aniso2.prepare_for_FFT(ds1, n=n_best, label=\"EP_1_aniso_1\")\n",
    "\n",
    "loss, ep1, sigep1, scale1 = aniso2.anisotropic_scaling_to_1_FFT_wilson_loss_fast(res[n_best].x, params, \\\n",
    "                                                        bUse=np.ones((ds1.shape[0],),dtype=bool), nargout=3)\n",
    "\n",
    "EP1_label = \"EP_1_aniso_2\"\n",
    "ds1[EP1_label]   = ep1\n",
    "ds1[\"SIG\" + EP1_label]= sigep1\n",
    "print(np.mean(ep1[ds1[\"CENTRIC\"]==True]**2))\n",
    "print(np.mean(ep1[ds1[\"CENTRIC\"]==False]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_cut = 0.1 # noisy observations deviation significantly from the Wilson distribution!\n",
    "              # ironically, French-Wilson scaling seems to blame.\n",
    "\n",
    "x=np.linspace(0,4,500)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(ep1[(ds1[\"CENTRIC\"]==True) & (sigep1<sig_cut)],20,alpha=0.5,density=True)\n",
    "plt.plot(x,wilson.wilson_dist_normalized(x,centric=True),'r-') \n",
    "plt.ylabel(\"count\"); plt.xlabel(\"Normalized sf amplitude\")\n",
    "plt.xlim([0,5]); plt.title(\"Centric\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(ep1[(ds1[\"CENTRIC\"]==False) & (sigep1<sig_cut)],40,alpha=0.5,density=True)\n",
    "plt.plot(x, wilson.wilson_dist_normalized(x,centric=False),'r-')\n",
    "plt.ylabel(\"count\"); plt.xlabel(\"Normalized sf amplitude\")\n",
    "plt.xlim([0,5]); plt.title(\"Acentric\"); \n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(scale1,100); plt.grid()\n",
    "plt.title(\"Scale factors v. naive aniso scaling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization by regression estimates of Sigma\n",
    "We're estimating $<|F|^2>$ locally in reciprocal space! Because the expected intensity is the same for acentric and centric reflections, we can do this in one go for all of them.\n",
    "\n",
    "By Rupp eq. (7-104): $ \\Sigma_N = \\left<I\\right>/\\varepsilon_h $\n",
    "\n",
    "Below are two implementations of $\\Sigma$ estimation: using kernel ridge regression and k-nearest neighbors with custom weights. The former retains more of the correlation among data sets than the latter, but is **much** slower and more memory-intensive. It crashes on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ds1[\"EPSILON\"][ds1[\"CENTRIC\"].to_numpy()==True]\n",
    "print(\"Distribution of multiplicities for centric reflections:\")\n",
    "print(temp.value_counts())\n",
    "print(\"\")\n",
    "\n",
    "temp = ds1[\"EPSILON\"][ds1[\"CENTRIC\"].to_numpy()==False]\n",
    "print(\"Distribution of multiplicities for acentric reflections:\")\n",
    "print(temp.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform k-nearest neighbor regression (with self excluded). The optimal $k$ (and uniform weights) are **much higher** when the input have already been anisotropically scaled. Larger $k$ is also slower..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = False \n",
    "ncpu=1 #8 worked on the cluster\n",
    "param_grid={\"n_neighbors\":[50,100,200,400,800],'weights':['uniform',knn.knn_weight_exp_p05, knn.knn_weight_norm_p05]}\n",
    "#                                                                   knn.knn_weight_exp_p02, knn.knn_weight_norm_p02, \\\n",
    "#                                                                   knn.knn_weight_exp_p03, knn.knn_weight_norm_p03]}\n",
    "if gridsearch: \n",
    "    knn_1 = GridSearchCV(KNeighborsRegressor(n_jobs=ncpu),param_grid=param_grid)\n",
    "    knn_1.fit(ds1[[\"rs_a\", \"rs_b\", \"rs_c\"]].to_numpy(), (ds1[EP1_label].to_numpy()**2)) # these should be corrected for eps already\n",
    "else:\n",
    "    knn_1 = KNeighborsRegressor(800, weights=knn.knn_weight_exp_p05,n_jobs=ncpu)\n",
    "    knn_1.fit(ds1[[\"rs_a\", \"rs_b\", \"rs_c\"]], (ds1[[EP1_label]].to_numpy()**2))\n",
    "\n",
    "if gridsearch:\n",
    "    print(knn_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_1 = knn_1.predict(ds1[[\"rs_a\", \"rs_b\", \"rs_c\"]]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E1    = ds1[        EP1_label].to_numpy().reshape(-1,1)/np.sqrt(Sigma_1) # We've already taken care of epsilon above\n",
    "SIGE1 = ds1[\"SIG\" + EP1_label].to_numpy().reshape(-1,1)/np.sqrt(Sigma_1)\n",
    "\n",
    "ds1[\"EP_1_knn\"]    = E1\n",
    "ds1[\"SIGEP_1_knn\"] = SIGE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,2,1)\n",
    "result, bin_labels = plots.compute_meanF_byres(ds1, label=\"FP\", nbins=20,sigma_cut=0)\n",
    "plots.plot_by_res_bin(result, bin_labels, ylabel=\"mean F\",color='b')\n",
    "plt.title(\"Mean |F|\")\n",
    "plt.ylim(0,)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "result_1, bin_labels_1 = plots.compute_meanF_byres(ds1, label=\"EP_1_aniso_1\", nbins=20,sigma_cut=0)\n",
    "result_2, bin_labels_2 = plots.compute_meanF_byres(ds1, label=\"EP_1_aniso_2\", nbins=20,sigma_cut=0)\n",
    "result_3, bin_labels_3 = plots.compute_meanF_byres(ds1, label=\"EP_1_knn\",     nbins=20,sigma_cut=0)\n",
    "plots.plot_by_res_bin(result_1, bin_labels_1, ylabel=\"mean E\",color='b')\n",
    "plots.plot_by_res_bin(result_2, bin_labels_2, ylabel=\"mean E\",color='r')\n",
    "plots.plot_by_res_bin(result_3, bin_labels_2, ylabel=\"mean E\",color='g')\n",
    "plt.title(\"Mean |F|\")\n",
    "plt.legend([\"EP1_aniso_1\", \"EP1_aniso_2\",\"EP_1_knn\"])\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the distribution of dHKL and E in the data\n",
    "sig_cut = 1\n",
    "nbin    = (20,10)\n",
    "\n",
    "cdf_bins     = np.linspace(0,100,nbin[0]+1)\n",
    "d_bins       = np.percentile(ds1[\"dHKL\"], cdf_bins)\n",
    "d_bins[-1]   = d_bins[-1]+1e-6   # to avoid omitting the largest data point\n",
    "d_dig        = np.digitize(ds1[\"dHKL\"], d_bins)\n",
    "ds1[\"dHKL_bin\"] = d_dig.flatten()\n",
    "# print(d_bins)\n",
    "\n",
    "cdf_bins     = np.linspace(0,100,nbin[1]+1)\n",
    "ep1_bins     = np.percentile(ds1[EP1_label], cdf_bins)\n",
    "ep1_bins[-1] = ep1_bins[-1]+1e-6 # to avoid omitting the largest data point\n",
    "ep1_dig      = np.digitize(ds1[\"EP_1_aniso_2\"], ep1_bins)\n",
    "ds1[\"EP_1_bin\"] = ep1_dig.flatten()\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(ds1.loc[(ds1[\"CENTRIC\"]==False) & (ds1[\"SIG\" + EP1_label]<sig_cut), \"dHKL_bin\"], \\\n",
    "                                   ds1.loc[(ds1[\"CENTRIC\"]==False) & (ds1[\"SIG\" + EP1_label]<sig_cut), \"EP_1_bin\"],bins=nbin)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "im=plt.imshow(H.transpose(), interpolation='nearest', origin='lower',\\\n",
    "           extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],vmin=0)\n",
    "plt.xlabel(\"resolution bin (high to low)\")\n",
    "plt.ylabel(\"|E| bin\")\n",
    "plt.title(\"Counts per 2D bin for |E| and dHKL\")\n",
    "plt.colorbar(im,fraction=0.025, pad=0.04)\n",
    "\n",
    "fname = mtz1 + \"_n\" + str(n_best) + \"_\" + EP1_label + \"_\" + str(rseed)+ \".png\"\n",
    "plt.savefig(\"results_figs/\" + fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1[\"rs_a\"]           = ds1[\"rs_a\"           ].astype(\"MTZReal\")\n",
    "ds1[\"rs_b\"]           = ds1[\"rs_b\"           ].astype(\"MTZReal\")\n",
    "ds1[\"rs_c\"]           = ds1[\"rs_c\"           ].astype(\"MTZReal\")\n",
    "ds1[\"EP_1_aniso_1\"]   = ds1[\"EP_1_aniso_1\"   ].astype(\"NormalizedSFAmplitude\")\n",
    "ds1[\"EP_1_aniso_2\"]   = ds1[\"EP_1_aniso_2\"   ].astype(\"NormalizedSFAmplitude\")\n",
    "ds1[\"EP_1_knn\"]       = ds1[\"EP_1_knn\"       ].astype(\"NormalizedSFAmplitude\")\n",
    "ds1[\"SIGEP_1_aniso_1\"]= ds1[\"SIGEP_1_aniso_1\"].astype(\"Stddev\")\n",
    "ds1[\"SIGEP_1_aniso_2\"]= ds1[\"SIGEP_1_aniso_2\"].astype(\"Stddev\")\n",
    "ds1[\"SIGEP_1_knn\"]    = ds1[\"SIGEP_1_knn\"    ].astype(\"Stddev\")\n",
    "# ds1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether we still have unsupported column datatypes around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_ds.check_col_dtypes(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have to add centric flags back later\n",
    "ds1.write_mtz(path + mtz1 + \"_scaled.mtz\",skip_problem_mtztypes=True)\n",
    "t_overall_end = perf_counter()\n",
    "print(\"Total elapsed time:\", t_overall_end-t_overall_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 4.5\n",
    "var = 0.5**2\n",
    "theta = var/mu\n",
    "k = mu/theta\n",
    "m = k\n",
    "Om = m*theta\n",
    "std = nakagami.std(m, loc=0, scale=np.sqrt(Om))\n",
    "x = gamma.rvs(a=k, scale=theta, size=10000)\n",
    "base=np.linspace(0,12,1000)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(x,50,density=True)\n",
    "plt.plot(base, gamma.pdf(x=base, a=k, scale=theta),'r-')\n",
    "plt.title(r\"Histogram for x.\"); plt.xlim([0,10])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.sqrt(x),50,density=True)\n",
    "print(np.mean(x))\n",
    "plt.plot(base, nakagami.pdf(x=base, nu=k, scale=np.sqrt(Om)),'r-')\n",
    "plt.title(r\"Histogram for $\\sqrt{x}$.\"); plt.xlim([0,5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-crystallography] *",
   "language": "python",
   "name": "conda-env-.conda-crystallography-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
